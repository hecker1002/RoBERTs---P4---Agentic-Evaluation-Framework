{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13048307,"sourceType":"datasetVersion","datasetId":8262656},{"sourceType":"datasetVersion","sourceId":13054375,"datasetId":8266562}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:35:51.800350Z","iopub.execute_input":"2025-09-14T09:35:51.800619Z","iopub.status.idle":"2025-09-14T09:35:52.095315Z","shell.execute_reply.started":"2025-09-14T09:35:51.800587Z","shell.execute_reply":"2025-09-14T09:35:52.094596Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/finaldataset/finals.csv\n/kaggle/input/hallucination-dataset/hallucination_dataset.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell: Clean Setup and Install (run this first, then RESTART KERNEL)\nimport os\nimport shutil\n\n# Clear any existing cache to avoid corruption\ncache_dir = '/root/.cache/huggingface'  # Default HF cache in Kaggle\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir)\n    print(\"Cleared Hugging Face cache\")\n\nworking_cache = '/kaggle/working/cache'\nif os.path.exists(working_cache):\n    shutil.rmtree(working_cache)\n    print(\"Cleared working cache\")\nos.makedirs(working_cache, exist_ok=True)\n\n# # Install a SINGLE stable version (no conflicts)\n# !pip uninstall -y transformers tokenizers  # Remove any broken installs first\n# !pip install transformers==4.41.2 datasets torch scikit-learn pandas numpy\n\n# print(\"Installation complete. NOW RESTART YOUR KERNEL (in Kaggle: Run > Restart Session) and continue from imports.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:35:52.096112Z","iopub.execute_input":"2025-09-14T09:35:52.096527Z","iopub.status.idle":"2025-09-14T09:35:52.102553Z","shell.execute_reply.started":"2025-09-14T09:35:52.096509Z","shell.execute_reply":"2025-09-14T09:35:52.101720Z"}},"outputs":[{"name":"stdout","text":"Cleared working cache\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install peft  # Add this if not installed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:35:52.103230Z","iopub.execute_input":"2025-09-14T09:35:52.104029Z","iopub.status.idle":"2025-09-14T09:35:55.576758Z","shell.execute_reply.started":"2025-09-14T09:35:52.104004Z","shell.execute_reply":"2025-09-14T09:35:55.575697Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.8.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.5.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom transformers import DistilBertModel, DistilBertTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:06.563611Z","iopub.execute_input":"2025-09-14T09:36:06.563927Z","iopub.status.idle":"2025-09-14T09:36:34.225530Z","shell.execute_reply.started":"2025-09-14T09:36:06.563900Z","shell.execute_reply":"2025-09-14T09:36:34.224916Z"}},"outputs":[{"name":"stderr","text":"2025-09-14 09:36:22.052409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757842582.263514      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757842582.328977      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Add this cell before dataset creation (cell 25)\n!rm -rf train_hallucination val_hallucination test_hallucination","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:03:59.677590Z","iopub.execute_input":"2025-09-14T06:03:59.677883Z","iopub.status.idle":"2025-09-14T06:03:59.825610Z","shell.execute_reply.started":"2025-09-14T06:03:59.677858Z","shell.execute_reply":"2025-09-14T06:03:59.824659Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:34.226622Z","iopub.execute_input":"2025-09-14T09:36:34.227178Z","iopub.status.idle":"2025-09-14T09:36:36.062959Z","shell.execute_reply.started":"2025-09-14T09:36:34.227158Z","shell.execute_reply":"2025-09-14T09:36:36.062337Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device='cuda'if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:36.063658Z","iopub.execute_input":"2025-09-14T09:36:36.063984Z","iopub.status.idle":"2025-09-14T09:36:36.068352Z","shell.execute_reply.started":"2025-09-14T09:36:36.063954Z","shell.execute_reply":"2025-09-14T09:36:36.067586Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:36.070015Z","iopub.execute_input":"2025-09-14T09:36:36.070793Z","iopub.status.idle":"2025-09-14T09:36:36.091337Z","shell.execute_reply.started":"2025-09-14T09:36:36.070774Z","shell.execute_reply":"2025-09-14T09:36:36.090502Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n# from transformers import AutoModel, Trainer, TrainingArguments\nfrom datasets import load_from_disk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:36.092153Z","iopub.execute_input":"2025-09-14T09:36:36.092355Z","iopub.status.idle":"2025-09-14T09:36:36.105720Z","shell.execute_reply.started":"2025-09-14T09:36:36.092332Z","shell.execute_reply":"2025-09-14T09:36:36.104996Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Verify version\nimport transformers\nprint(f\"Transformers version: {transformers.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:55.483810Z","iopub.execute_input":"2025-09-14T09:36:55.484442Z","iopub.status.idle":"2025-09-14T09:36:55.488732Z","shell.execute_reply.started":"2025-09-14T09:36:55.484417Z","shell.execute_reply":"2025-09-14T09:36:55.487902Z"}},"outputs":[{"name":"stdout","text":"Transformers version: 4.52.4\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:56.332530Z","iopub.execute_input":"2025-09-14T09:36:56.333211Z","iopub.status.idle":"2025-09-14T09:36:56.336812Z","shell.execute_reply.started":"2025-09-14T09:36:56.333185Z","shell.execute_reply":"2025-09-14T09:36:56.335981Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/finaldataset/finals.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:57.111731Z","iopub.execute_input":"2025-09-14T09:36:57.112281Z","iopub.status.idle":"2025-09-14T09:36:57.128193Z","shell.execute_reply.started":"2025-09-14T09:36:57.112256Z","shell.execute_reply":"2025-09-14T09:36:57.127562Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:57.857545Z","iopub.execute_input":"2025-09-14T09:36:57.858432Z","iopub.status.idle":"2025-09-14T09:36:57.866240Z","shell.execute_reply.started":"2025-09-14T09:36:57.858404Z","shell.execute_reply":"2025-09-14T09:36:57.865380Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Index(['prompt', 'response', 'agent_type', 'Conciseness Score',\n       'Instruction-Following Score', 'Persona Adherence Score',\n       'Assumption Control Score', 'user_frustration_proxy', 'overconfidence',\n       'semantic_alignment', 'semantic_drift', 'repetition_percentage',\n       'contradiction_detected', 'assumption_risk', 'sentence_variety_score',\n       'keyword_recall', 'extraneous_ratio', 'alignment_balance',\n       'spelling_errors', 'number_mismatch_count'],\n      dtype='object')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:36:59.189588Z","iopub.execute_input":"2025-09-14T09:36:59.190316Z","iopub.status.idle":"2025-09-14T09:36:59.200939Z","shell.execute_reply.started":"2025-09-14T09:36:59.190289Z","shell.execute_reply":"2025-09-14T09:36:59.200096Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"prompt                         0\nresponse                       0\nagent_type                     0\nConciseness Score              0\nInstruction-Following Score    0\nPersona Adherence Score        0\nAssumption Control Score       0\nuser_frustration_proxy         0\noverconfidence                 0\nsemantic_alignment             0\nsemantic_drift                 0\nrepetition_percentage          0\ncontradiction_detected         0\nassumption_risk                0\nsentence_variety_score         0\nkeyword_recall                 0\nextraneous_ratio               0\nalignment_balance              0\nspelling_errors                0\nnumber_mismatch_count          0\ndtype: int64"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:04:17.157464Z","iopub.execute_input":"2025-09-14T06:04:17.157916Z","iopub.status.idle":"2025-09-14T06:04:17.162369Z","shell.execute_reply.started":"2025-09-14T06:04:17.157895Z","shell.execute_reply":"2025-09-14T06:04:17.161797Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"35"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# df=df[:15]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:04:17.996080Z","iopub.execute_input":"2025-09-14T06:04:17.996491Z","iopub.status.idle":"2025-09-14T06:04:17.999664Z","shell.execute_reply.started":"2025-09-14T06:04:17.996471Z","shell.execute_reply":"2025-09-14T06:04:17.998834Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Model pre processing","metadata":{}},{"cell_type":"code","source":"def bert_input_string(idx: int, data: pd.DataFrame) -> str:\n    \"\"\"\n    Generates a BERT prompt string by fetching all required features from a\n    single row of a pandas DataFrame.\n\n    Args:\n        idx (int): The index of the row to fetch data from.\n        data (pd.DataFrame): The DataFrame containing the input features.\n\n    Returns:\n        str: The formatted prompt string ready for a BERT model.\n    \"\"\"\n    # Fetch the row at the specified index\n    row = data.iloc[idx]\n\n    # Extract all the features from the row\n# Extract all the features from the row\n    prompt = row['prompt']\n    response = row['response']\n    user_frustration_proxy = row['user_frustration_proxy']\n    overconfidence = row['overconfidence']\n    semantic_alignment = row['semantic_alignment']\n    semantic_drift = row['semantic_drift'] # Not used in prompt\n    repetition_percentage = row['repetition_percentage']\n    contradiction_detected = row['contradiction_detected']\n    assumption_risk = row['assumption_risk']\n    sentence_variety_score = row['sentence_variety_score'] # Not used in prompt\n    keyword_recall = row['keyword_recall']\n    extraneous_ratio = row['extraneous_ratio'] # Not used in prompt\n    spelling_errors = row['spelling_errors']\n    \n    # --- Refined Prompt ---\n    input_bert = f\"\"\"\n    [CONTEXT]\n    Analyze the following user-AI interaction based on the provided text and pre-computed quality metrics.\n    \n    [USER PROMPT]\n    {prompt}\n    \n    [AI RESPONSE]\n    {response}\n    \n    [QUALITY METRICS]\n    - User Frustration Proxy: {user_frustration_proxy:.2%}\n    - Overconfidence Score: {overconfidence:.2%}\n    - Semantic Alignment Score: {semantic_alignment:.2%}\n    - Repetition Percentage: {repetition_percentage:.2%}\n    - Assumption Risk Score: {assumption_risk:.2%}\n    - Keyword Recall: {keyword_recall:.2%}\n    - Spelling Error Count: {spelling_errors}\n\n    [TASK]\n    Based on all the information above, predict the following score:\n    - Instruction-Following Score\n    \"\"\" \n    \n    return input_bert\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:03.417391Z","iopub.execute_input":"2025-09-14T09:37:03.418118Z","iopub.status.idle":"2025-09-14T09:37:03.424324Z","shell.execute_reply.started":"2025-09-14T09:37:03.418094Z","shell.execute_reply":"2025-09-14T09:37:03.423587Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df['input_string'] = [bert_input_string(i, df) for i in range(len(df))]\ndf['hallucination_score'] = df['Instruction-Following Score'].astype(float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:06.439712Z","iopub.execute_input":"2025-09-14T09:37:06.440344Z","iopub.status.idle":"2025-09-14T09:37:06.450451Z","shell.execute_reply.started":"2025-09-14T09:37:06.440312Z","shell.execute_reply":"2025-09-14T09:37:06.449655Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport pandas as pd\n\ntrain_val, test = train_test_split(df, test_size=0.1, random_state=42)\ntrain, val = train_test_split(train_val, test_size=0.1111, random_state=42)\ntrain = train.rename(columns={'hallucination_score': 'labels'})\nval = val.rename(columns={'hallucination_score': 'labels'})\ntest = test.rename(columns={'hallucination_score': 'labels'})\ntrain_dataset = Dataset.from_pandas(train[['input_string', 'labels']].reset_index(drop=True))\nval_dataset = Dataset.from_pandas(val[['input_string', 'labels']].reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test[['input_string', 'labels']].reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:08.955838Z","iopub.execute_input":"2025-09-14T09:37:08.956167Z","iopub.status.idle":"2025-09-14T09:37:09.003720Z","shell.execute_reply.started":"2025-09-14T09:37:08.956146Z","shell.execute_reply":"2025-09-14T09:37:09.002990Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize\n\nfrom transformers import BertTokenizer  # Switch from DistilBertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('gaunernst/bert-mini-uncased', cache_dir='/kaggle/working/cache')\n# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', cache_dir='/kaggle/working/cache')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:11.547488Z","iopub.execute_input":"2025-09-14T09:37:11.547799Z","iopub.status.idle":"2025-09-14T09:37:12.510695Z","shell.execute_reply.started":"2025-09-14T09:37:11.547774Z","shell.execute_reply":"2025-09-14T09:37:12.510108Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/32.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"590c3b134a4245f39e34ed17fbedee70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8126a9ac737e41e389df20f5800d2c5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"646f66bb5ce247aba041a423998feef7"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples['input_string'], truncation=True, padding='max_length', max_length=256)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:15.395599Z","iopub.execute_input":"2025-09-14T09:37:15.395941Z","iopub.status.idle":"2025-09-14T09:37:15.400115Z","shell.execute_reply.started":"2025-09-14T09:37:15.395907Z","shell.execute_reply":"2025-09-14T09:37:15.399300Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:19.268535Z","iopub.execute_input":"2025-09-14T09:37:19.268831Z","iopub.status.idle":"2025-09-14T09:37:20.053499Z","shell.execute_reply.started":"2025-09-14T09:37:19.268807Z","shell.execute_reply":"2025-09-14T09:37:20.052585Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ca7a771b1746f499effa3390fbfe7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73ef4bb4f104bca9caf2d25ddada53f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a0a8e0a9cf748bfb39796cb62547d46"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"print(\"Train dataset features:\", train_dataset.features)\nprint(\"First train sample:\", train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:22.184775Z","iopub.execute_input":"2025-09-14T09:37:22.185078Z","iopub.status.idle":"2025-09-14T09:37:22.192163Z","shell.execute_reply.started":"2025-09-14T09:37:22.185056Z","shell.execute_reply":"2025-09-14T09:37:22.191388Z"}},"outputs":[{"name":"stdout","text":"Train dataset features: {'input_string': Value(dtype='string', id=None), 'labels': Value(dtype='float64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\nFirst train sample: {'input_string': \"\\n    [CONTEXT]\\n    Analyze the following user-AI interaction based on the provided text and pre-computed quality metrics.\\n    \\n    [USER PROMPT]\\n    Name the capital of Brazil and the two largest rivers that flow through the country.\\n    \\n    [AI RESPONSE]\\n    Brasília is the capital of Brazil. The Amazon River and the Paraná River are the two largest rivers flowing through the country. The Amazon, the world's largest by discharge volume, traverses the northern region. The Paraná, South America's second-longest river system, flows through or borders the southern parts of the nation.\\n    \\n    [QUALITY METRICS]\\n    - User Frustration Proxy: 69.60%\\n    - Overconfidence Score: 76.30%\\n    - Semantic Alignment Score: 94.90%\\n    - Repetition Percentage: 0.00%\\n    - Assumption Risk Score: 0.00%\\n    - Keyword Recall: 100.00%\\n    - Spelling Error Count: 0\\n\\n    [TASK]\\n    Based on all the information above, predict the following score:\\n    - Instruction-Following Score\\n    \", 'labels': 8.0, 'input_ids': [101, 1031, 6123, 1033, 17908, 1996, 2206, 5310, 1011, 9932, 8290, 2241, 2006, 1996, 3024, 3793, 1998, 3653, 1011, 24806, 3737, 12046, 2015, 1012, 1031, 5310, 25732, 1033, 2171, 1996, 3007, 1997, 4380, 1998, 1996, 2048, 2922, 5485, 2008, 4834, 2083, 1996, 2406, 1012, 1031, 9932, 3433, 1033, 21133, 2401, 2003, 1996, 3007, 1997, 4380, 1012, 1996, 9733, 2314, 1998, 1996, 28383, 2314, 2024, 1996, 2048, 2922, 5485, 8577, 2083, 1996, 2406, 1012, 1996, 9733, 1010, 1996, 2088, 1005, 1055, 2922, 2011, 11889, 3872, 1010, 20811, 2015, 1996, 2642, 2555, 1012, 1996, 28383, 1010, 2148, 2637, 1005, 1055, 2117, 1011, 6493, 2314, 2291, 1010, 6223, 2083, 2030, 6645, 1996, 2670, 3033, 1997, 1996, 3842, 1012, 1031, 3737, 12046, 2015, 1033, 1011, 5310, 9135, 24540, 1024, 6353, 1012, 3438, 1003, 1011, 2058, 8663, 20740, 5897, 3556, 1024, 6146, 1012, 2382, 1003, 1011, 21641, 12139, 3556, 1024, 6365, 1012, 3938, 1003, 1011, 23318, 7017, 1024, 1014, 1012, 4002, 1003, 1011, 11213, 3891, 3556, 1024, 1014, 1012, 4002, 1003, 1011, 3145, 18351, 9131, 1024, 2531, 1012, 4002, 1003, 1011, 11379, 7561, 4175, 1024, 1014, 1031, 4708, 1033, 2241, 2006, 2035, 1996, 2592, 2682, 1010, 16014, 1996, 2206, 3556, 1024, 1011, 7899, 1011, 2206, 3556, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Check data input format","metadata":{}},{"cell_type":"code","source":"# train_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:04:30.917586Z","iopub.execute_input":"2025-09-14T06:04:30.917856Z","iopub.status.idle":"2025-09-14T06:04:30.921334Z","shell.execute_reply.started":"2025-09-14T06:04:30.917835Z","shell.execute_reply":"2025-09-14T06:04:30.920587Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Set PyTorch format\n# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'hallucination_score'])\n# val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'hallucination_score'])\n# test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'hallucination_score'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:04:31.127167Z","iopub.execute_input":"2025-09-14T06:04:31.127346Z","iopub.status.idle":"2025-09-14T06:04:31.130473Z","shell.execute_reply.started":"2025-09-14T06:04:31.127329Z","shell.execute_reply":"2025-09-14T06:04:31.129648Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After tokenization (cell 28)\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\nval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\nprint(\"Train dataset after set_format:\", train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:28.576683Z","iopub.execute_input":"2025-09-14T09:37:28.577061Z","iopub.status.idle":"2025-09-14T09:37:28.604830Z","shell.execute_reply.started":"2025-09-14T09:37:28.577037Z","shell.execute_reply":"2025-09-14T09:37:28.604161Z"}},"outputs":[{"name":"stdout","text":"Train dataset after set_format: {'labels': tensor(8.), 'input_ids': tensor([  101,  1031,  6123,  1033, 17908,  1996,  2206,  5310,  1011,  9932,\n         8290,  2241,  2006,  1996,  3024,  3793,  1998,  3653,  1011, 24806,\n         3737, 12046,  2015,  1012,  1031,  5310, 25732,  1033,  2171,  1996,\n         3007,  1997,  4380,  1998,  1996,  2048,  2922,  5485,  2008,  4834,\n         2083,  1996,  2406,  1012,  1031,  9932,  3433,  1033, 21133,  2401,\n         2003,  1996,  3007,  1997,  4380,  1012,  1996,  9733,  2314,  1998,\n         1996, 28383,  2314,  2024,  1996,  2048,  2922,  5485,  8577,  2083,\n         1996,  2406,  1012,  1996,  9733,  1010,  1996,  2088,  1005,  1055,\n         2922,  2011, 11889,  3872,  1010, 20811,  2015,  1996,  2642,  2555,\n         1012,  1996, 28383,  1010,  2148,  2637,  1005,  1055,  2117,  1011,\n         6493,  2314,  2291,  1010,  6223,  2083,  2030,  6645,  1996,  2670,\n         3033,  1997,  1996,  3842,  1012,  1031,  3737, 12046,  2015,  1033,\n         1011,  5310,  9135, 24540,  1024,  6353,  1012,  3438,  1003,  1011,\n         2058,  8663, 20740,  5897,  3556,  1024,  6146,  1012,  2382,  1003,\n         1011, 21641, 12139,  3556,  1024,  6365,  1012,  3938,  1003,  1011,\n        23318,  7017,  1024,  1014,  1012,  4002,  1003,  1011, 11213,  3891,\n         3556,  1024,  1014,  1012,  4002,  1003,  1011,  3145, 18351,  9131,\n         1024,  2531,  1012,  4002,  1003,  1011, 11379,  7561,  4175,  1024,\n         1014,  1031,  4708,  1033,  2241,  2006,  2035,  1996,  2592,  2682,\n         1010, 16014,  1996,  2206,  3556,  1024,  1011,  7899,  1011,  2206,\n         3556,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!rm -rf train_hallucination val_hallucination test_hallucination","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:04:32.807165Z","iopub.execute_input":"2025-09-14T06:04:32.807340Z","iopub.status.idle":"2025-09-14T06:04:32.954366Z","shell.execute_reply.started":"2025-09-14T06:04:32.807326Z","shell.execute_reply":"2025-09-14T06:04:32.953613Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"print(train_dataset.features)  # Should show 'labels': FloatField or similar\nprint(train_dataset[0])  # Should include 'labels' as a tensor or value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:33.988228Z","iopub.execute_input":"2025-09-14T09:37:33.988484Z","iopub.status.idle":"2025-09-14T09:37:33.998280Z","shell.execute_reply.started":"2025-09-14T09:37:33.988468Z","shell.execute_reply":"2025-09-14T09:37:33.997495Z"}},"outputs":[{"name":"stdout","text":"{'input_string': Value(dtype='string', id=None), 'labels': Value(dtype='float64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n{'labels': tensor(8.), 'input_ids': tensor([  101,  1031,  6123,  1033, 17908,  1996,  2206,  5310,  1011,  9932,\n         8290,  2241,  2006,  1996,  3024,  3793,  1998,  3653,  1011, 24806,\n         3737, 12046,  2015,  1012,  1031,  5310, 25732,  1033,  2171,  1996,\n         3007,  1997,  4380,  1998,  1996,  2048,  2922,  5485,  2008,  4834,\n         2083,  1996,  2406,  1012,  1031,  9932,  3433,  1033, 21133,  2401,\n         2003,  1996,  3007,  1997,  4380,  1012,  1996,  9733,  2314,  1998,\n         1996, 28383,  2314,  2024,  1996,  2048,  2922,  5485,  8577,  2083,\n         1996,  2406,  1012,  1996,  9733,  1010,  1996,  2088,  1005,  1055,\n         2922,  2011, 11889,  3872,  1010, 20811,  2015,  1996,  2642,  2555,\n         1012,  1996, 28383,  1010,  2148,  2637,  1005,  1055,  2117,  1011,\n         6493,  2314,  2291,  1010,  6223,  2083,  2030,  6645,  1996,  2670,\n         3033,  1997,  1996,  3842,  1012,  1031,  3737, 12046,  2015,  1033,\n         1011,  5310,  9135, 24540,  1024,  6353,  1012,  3438,  1003,  1011,\n         2058,  8663, 20740,  5897,  3556,  1024,  6146,  1012,  2382,  1003,\n         1011, 21641, 12139,  3556,  1024,  6365,  1012,  3938,  1003,  1011,\n        23318,  7017,  1024,  1014,  1012,  4002,  1003,  1011, 11213,  3891,\n         3556,  1024,  1014,  1012,  4002,  1003,  1011,  3145, 18351,  9131,\n         1024,  2531,  1012,  4002,  1003,  1011, 11379,  7561,  4175,  1024,\n         1014,  1031,  4708,  1033,  2241,  2006,  2035,  1996,  2592,  2682,\n         1010, 16014,  1996,  2206,  3556,  1024,  1011,  7899,  1011,  2206,\n         3556,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save datasets\ntrain_dataset.save_to_disk('train_hallucination')\nval_dataset.save_to_disk('val_hallucination')\ntest_dataset.save_to_disk('test_hallucination')\n\nprint(\"Preprocessing complete. Datasets saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:37.035377Z","iopub.execute_input":"2025-09-14T09:37:37.036068Z","iopub.status.idle":"2025-09-14T09:37:37.108473Z","shell.execute_reply.started":"2025-09-14T09:37:37.036042Z","shell.execute_reply":"2025-09-14T09:37:37.107728Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/14 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"456c3e04052c4ff39d21611227d852cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9f017c5c334695b051fa0ffc8c0e4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a770c796ae548c1baf574874bcc5fa2"}},"metadata":{}},{"name":"stdout","text":"Preprocessing complete. Datasets saved.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from transformers import BertModel, BertTokenizer  # Switch to Bert* for bert-mini-uncased (explained in section 2)\n# from peft import LoraConfig, get_peft_model  # For LoRA\n\n# class HallucinationRegressor(nn.Module):\n#     def __init__(self, base_model_name='gaunernst/bert-mini-uncased', use_lora=False, lora_rank=8):\n#         super().__init__()\n#         try:\n#             self.base_model = BertModel.from_pretrained(\n#                 base_model_name,\n#                 cache_dir='/kaggle/working/cache'\n#             )\n#             if use_lora:\n#                 # Configure LoRA (target attention layers for efficiency)\n#                 peft_config = LoraConfig(\n#                     r=lora_rank,  # Low-rank dimension (8-32 typical; higher=better quality, more params)\n#                     lora_alpha=16,  # Scaling factor\n#                     lora_dropout=0.1,\n#                     bias=\"none\",\n#                     target_modules=[\"query\", \"key\", \"value\"]  # Focus on attention for BERT-like models\n#                 )\n#                 self.base_model = get_peft_model(self.base_model, peft_config)\n#                 self.base_model.print_trainable_parameters()  # Logs % trainable params (should be low)\n#                 print(f\"LoRA enabled with rank {lora_rank}\")\n#             print(f\"Successfully loaded {base_model_name}\")\n#         except Exception as e:\n#             print(f\"Failed to load model: {e}\")\n#             raise\n#         # Regression head on pooled output\n#         self.regression_head = nn.Linear(self.base_model.config.hidden_size, 1)\n\n#     def forward(self, input_ids, attention_mask):\n#         outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n#         pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token pooling\n#         hallucination_score = self.regression_head(pooled_output).squeeze(-1)\n#         return {'hallucination_score': hallucination_score}\n\n# # class HallucinationRegressor(nn.Module):\n# #     def __init__(self, base_model_name='distilbert-base-uncased'):\n# #         super().__init__()\n# #         try:\n# #             # Load directly with DistilBertModel to bypass AutoModel issues\n# #             self.bert = DistilBertModel.from_pretrained(\n# #                 base_model_name,\n# #                 cache_dir='/kaggle/working/cache'  # Use your cleared working cache\n# #             )\n# #             print(f\"Successfully loaded {base_model_name}\")\n# #         except Exception as e:\n# #             print(f\"Failed to load model: {e}\")\n# #             raise\n# #         self.regression_head = nn.Linear(self.bert.config.hidden_size, 1)\n\n# #     def forward(self, input_ids, attention_mask):\n# #         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n# #         # Use [CLS] token for pooling (DistilBERT uses last_hidden_state[:, 0, :])\n# #         pooled_output = outputs.last_hidden_state[:, 0]\n# #         hallucination_score = self.regression_head(pooled_output).squeeze(-1)\n# #         return {'hallucination_score': hallucination_score}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T20:52:53.599589Z","iopub.execute_input":"2025-09-13T20:52:53.599882Z","iopub.status.idle":"2025-09-13T20:52:53.609488Z","shell.execute_reply.started":"2025-09-13T20:52:53.599854Z","shell.execute_reply":"2025-09-13T20:52:53.608800Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"# FIXED VERSION - Remove the extra 'self' parameter\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer  # Switch to Bert* for bert-mini-uncased (explained in section 2)\nfrom peft import LoraConfig, get_peft_model  # For LoRA\nclass HallucinationRegressor(nn.Module):\n    def __init__(self, base_model_name='gaunernst/bert-mini-uncased', use_lora=False, lora_rank=8):\n        super().__init__()  # Remove the 'self' parameter here\n        try:\n            self.base_model = BertModel.from_pretrained(\n                base_model_name,\n                cache_dir='/kaggle/working/cache'\n            )\n            if use_lora:\n                peft_config = LoraConfig(\n                    r=lora_rank,\n                    lora_alpha=16,\n                    lora_dropout=0.1,\n                    bias=\"none\",\n                    target_modules=[\"query\", \"key\", \"value\"]\n                )\n                self.base_model = get_peft_model(self.base_model, peft_config)\n                self.base_model.print_trainable_parameters()\n                print(f\"LoRA enabled with rank {lora_rank}\")\n            print(f\"Successfully loaded {base_model_name}\")\n        except Exception as e:\n            print(f\"Failed to load model: {e}\")\n            raise\n        self.regression_head = nn.Linear(self.base_model.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        logits = self.regression_head(pooled_output).squeeze(-1)\n        \n        loss = None\n        if labels is not None:\n            loss = nn.MSELoss()(logits, labels)\n        \n        return {\"loss\": loss, \"logits\": logits}\n\n# Now this should work:\n# model = HallucinationRegressor(use_lora=True)\n\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n# )\n\n# trainer.train()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:41.145582Z","iopub.execute_input":"2025-09-14T09:37:41.145912Z","iopub.status.idle":"2025-09-14T09:37:41.182270Z","shell.execute_reply.started":"2025-09-14T09:37:41.145857Z","shell.execute_reply":"2025-09-14T09:37:41.181695Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel\nfrom peft import LoraConfig, get_peft_model, TaskType  # Ensure TaskType is imported\n\nclass HallucinationRegressor(nn.Module):\n    def __init__(self, use_lora=False, lora_rank=8):\n        super().__init__()\n        self.base_model = BertModel.from_pretrained('gaunernst/bert-mini-uncased')\n        self.regression_head = nn.Linear(self.base_model.config.hidden_size, 1)\n        \n        if use_lora:\n            lora_config = LoraConfig(\n                task_type=TaskType.FEATURE_EXTRACTION,  # <-- CHANGE HERE: Use FEATURE_EXTRACTION for encoder-only\n                inference_mode=False,\n                r=lora_rank,\n                lora_alpha=32,\n                lora_dropout=0.1,\n                target_modules=[\"query\", \"value\", \"key\"]\n            )\n            self.base_model = get_peft_model(self.base_model, lora_config)\n            self.base_model.print_trainable_parameters()\n        \n        # Freeze base unless LoRA (already handled by PEFT)\n    \n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n        if token_type_ids is not None:\n            del token_type_ids  # Avoid passing unused token_type_ids to base_model\n        \n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        logits = self.regression_head(pooled_output)\n        \n        # Compute loss only if labels provided (for Trainer compatibility)\n        loss = None\n        if labels is not None:\n            loss = nn.MSELoss()(logits, labels)\n        \n        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:44.296651Z","iopub.execute_input":"2025-09-14T09:37:44.297285Z","iopub.status.idle":"2025-09-14T09:37:44.304476Z","shell.execute_reply.started":"2025-09-14T09:37:44.297258Z","shell.execute_reply":"2025-09-14T09:37:44.303657Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_from_disk('train_hallucination')\nval_dataset = load_from_disk('val_hallucination')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:47.241454Z","iopub.execute_input":"2025-09-14T09:37:47.242217Z","iopub.status.idle":"2025-09-14T09:37:47.253360Z","shell.execute_reply.started":"2025-09-14T09:37:47.242191Z","shell.execute_reply":"2025-09-14T09:37:47.252687Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"train_dataset = train_dataset.with_format(\"torch\")\nval_dataset = val_dataset.with_format(\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:49.303597Z","iopub.execute_input":"2025-09-14T09:37:49.304193Z","iopub.status.idle":"2025-09-14T09:37:49.309590Z","shell.execute_reply.started":"2025-09-14T09:37:49.304170Z","shell.execute_reply":"2025-09-14T09:37:49.308923Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(train_dataset.features)  # Should show 'labels': FloatField or similar\nprint(train_dataset[0])  # Should include 'labels' as a tensor or value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:09:46.787470Z","iopub.execute_input":"2025-09-14T06:09:46.787759Z","iopub.status.idle":"2025-09-14T06:09:46.797091Z","shell.execute_reply.started":"2025-09-14T06:09:46.787736Z","shell.execute_reply":"2025-09-14T06:09:46.796430Z"}},"outputs":[{"name":"stdout","text":"{'input_string': Value(dtype='string', id=None), 'labels': Value(dtype='float64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n{'input_string': 'We have Response prompt pairs , where prompt is Explain quantum mechanics simply and\\n    the corresponding response is Quantum mechanics studies how particles like electrons behave at tiny scales, often acting like waves and particles simultaneously. where the feat 1 is 0.45\\n    give me the hallucination score as output', 'labels': tensor(0.1000), 'input_ids': tensor([  101,  2057,  2031,  3433, 25732,  7689,  1010,  2073, 25732,  2003,\n         4863,  8559,  9760,  3432,  1998,  1996,  7978,  3433,  2003,  8559,\n         9760,  2913,  2129,  9309,  2066, 15057, 16582,  2012,  4714,  9539,\n         1010,  2411,  3772,  2066,  5975,  1998,  9309,  7453,  1012,  2073,\n         1996,  8658,  1015,  2003,  1014,  1012,  3429,  2507,  2033,  1996,\n         2534, 14194, 12758,  3556,  2004,  6434,   102,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# Loss function\n# def compute_loss(model, inputs, return_outputs=False):\n#     labels = inputs.pop('hallucination_score').to(model.device)\n#     outputs = model(**inputs)\n#     loss = nn.MSELoss()(outputs['hallucination_score'], labels)\n#     return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:01:47.621170Z","iopub.execute_input":"2025-09-13T21:01:47.621432Z","iopub.status.idle":"2025-09-13T21:01:47.625993Z","shell.execute_reply.started":"2025-09-13T21:01:47.621414Z","shell.execute_reply":"2025-09-13T21:01:47.625108Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"# def compute_loss(model, inputs, return_outputs=False):\n#     print(\"Inputs to compute_loss:\", inputs.keys())  # Debug\n#     labels = inputs.pop('labels').to(model.device)\n#     outputs = model(**inputs)\n#     loss = nn.MSELoss()(outputs['hallucination_score'], labels)\n#     return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:11:57.716488Z","iopub.execute_input":"2025-09-14T06:11:57.717170Z","iopub.status.idle":"2025-09-14T06:11:57.721170Z","shell.execute_reply.started":"2025-09-14T06:11:57.717145Z","shell.execute_reply":"2025-09-14T06:11:57.720450Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Update Cell 34 to use 'logits' (not 'hallucination_score')\ndef compute_loss(model, inputs, return_outputs=False):\n    labels = inputs.pop('labels').to(model.device)\n    outputs = model(**inputs)\n    loss = nn.MSELoss()(outputs['logits'], labels)  # Fix key to 'logits'\n    return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:37:53.497508Z","iopub.execute_input":"2025-09-14T09:37:53.497763Z","iopub.status.idle":"2025-09-14T09:37:53.502655Z","shell.execute_reply.started":"2025-09-14T09:37:53.497745Z","shell.execute_reply":"2025-09-14T09:37:53.501730Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/hallucination_results',\n    num_train_epochs=48,\n    per_device_train_batch_size=5,\n    per_device_eval_batch_size=5,\n    warmup_steps=10,\n    gradient_accumulation_steps=1,\n    weight_decay=0.01,\n    logging_dir='/kaggle/working/hallucination_logs',\n    logging_steps=10,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    learning_rate=5e-4,\n    fp16=True,\n    report_to=\"none\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:42:53.905407Z","iopub.execute_input":"2025-09-14T09:42:53.906039Z","iopub.status.idle":"2025-09-14T09:42:53.944577Z","shell.execute_reply.started":"2025-09-14T09:42:53.906008Z","shell.execute_reply":"2025-09-14T09:42:53.943788Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"def custom_data_collator(features):\n    \"\"\"Custom collator that preserves labels\"\"\"\n    batch = {}\n    batch['input_ids'] = torch.stack([f['input_ids'] for f in features])\n    batch['attention_mask'] = torch.stack([f['attention_mask'] for f in features])\n    batch['labels'] = torch.stack([f['labels'] for f in features])\n    return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:42:54.715562Z","iopub.execute_input":"2025-09-14T09:42:54.715838Z","iopub.status.idle":"2025-09-14T09:42:54.720750Z","shell.execute_reply.started":"2025-09-14T09:42:54.715818Z","shell.execute_reply":"2025-09-14T09:42:54.719804Z"}},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"## Custom trainer","metadata":{}},{"cell_type":"code","source":"# # Custom Trainer\n# class CustomTrainer(Trainer):\n#     def compute_loss(self, model, inputs, return_outputs=False):\n#         return compute_loss(model, inputs, return_outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:42:55.481463Z","iopub.execute_input":"2025-09-14T09:42:55.481751Z","iopub.status.idle":"2025-09-14T09:42:55.485455Z","shell.execute_reply.started":"2025-09-14T09:42:55.481730Z","shell.execute_reply":"2025-09-14T09:42:55.484734Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"# from transformers import Trainer\n\n# class CustomTrainer(Trainer):\n#     def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n#         return compute_loss(model, inputs, return_outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:42:56.640559Z","iopub.execute_input":"2025-09-14T09:42:56.640841Z","iopub.status.idle":"2025-09-14T09:42:56.644727Z","shell.execute_reply.started":"2025-09-14T09:42:56.640821Z","shell.execute_reply":"2025-09-14T09:42:56.643919Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# # print(\"Dataset sample:\", train_dataset[0])\n# print(\"Dataset columns after set_format:\", list(train_dataset[0].keys()))\n# sample_batch = data_collator([train_dataset[0], train_dataset[1]])\n# print(\"Collated batch keys:\", list(sample_batch.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:42:57.980645Z","iopub.execute_input":"2025-09-14T09:42:57.981385Z","iopub.status.idle":"2025-09-14T09:42:57.984326Z","shell.execute_reply.started":"2025-09-14T09:42:57.981358Z","shell.execute_reply":"2025-09-14T09:42:57.983644Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer, BertConfig\nfrom peft import LoraConfig, get_peft_model, TaskType","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:42:58.541598Z","iopub.execute_input":"2025-09-14T09:42:58.542064Z","iopub.status.idle":"2025-09-14T09:42:58.545830Z","shell.execute_reply.started":"2025-09-14T09:42:58.542041Z","shell.execute_reply":"2025-09-14T09:42:58.545178Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"model = HallucinationRegressor(use_lora=True)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:42:59.242761Z","iopub.execute_input":"2025-09-14T09:42:59.243372Z","iopub.status.idle":"2025-09-14T09:43:11.250572Z","shell.execute_reply.started":"2025-09-14T09:42:59.243345Z","shell.execute_reply":"2025-09-14T09:43:11.249999Z"}},"outputs":[{"name":"stdout","text":"trainable params: 49,152 || all params: 11,219,712 || trainable%: 0.4381\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [96/96 00:11, Epoch 48/48]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>82.789543</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>81.451683</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>78.930336</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>75.018799</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>61.667200</td>\n      <td>69.354630</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>61.667200</td>\n      <td>62.138542</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>61.667200</td>\n      <td>55.649681</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>61.667200</td>\n      <td>51.059120</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>61.667200</td>\n      <td>47.771805</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>45.530500</td>\n      <td>44.947052</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>45.530500</td>\n      <td>42.354870</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>45.530500</td>\n      <td>40.057014</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>45.530500</td>\n      <td>37.908203</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>45.530500</td>\n      <td>35.778969</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>30.766500</td>\n      <td>33.732391</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>30.766500</td>\n      <td>31.813366</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>30.766500</td>\n      <td>30.069111</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>30.766500</td>\n      <td>28.511688</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>30.766500</td>\n      <td>27.021877</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>22.220400</td>\n      <td>25.637085</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>22.220400</td>\n      <td>24.338785</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>22.220400</td>\n      <td>23.107121</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>22.220400</td>\n      <td>21.992249</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>22.220400</td>\n      <td>20.986250</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>16.961200</td>\n      <td>20.032547</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>16.961200</td>\n      <td>19.136059</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>16.961200</td>\n      <td>18.274111</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>16.961200</td>\n      <td>17.461922</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>16.961200</td>\n      <td>16.715481</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>13.597500</td>\n      <td>16.038979</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>13.597500</td>\n      <td>15.416554</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>13.597500</td>\n      <td>14.839019</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>13.597500</td>\n      <td>14.310478</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>13.597500</td>\n      <td>13.823582</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>10.916900</td>\n      <td>13.367225</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>10.916900</td>\n      <td>12.942493</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>10.916900</td>\n      <td>12.555920</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>10.916900</td>\n      <td>12.210182</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>10.916900</td>\n      <td>11.906526</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>10.019700</td>\n      <td>11.640856</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>10.019700</td>\n      <td>11.408796</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>10.019700</td>\n      <td>11.206953</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>10.019700</td>\n      <td>11.038471</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>10.019700</td>\n      <td>10.901005</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>8.532600</td>\n      <td>10.793736</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>8.532600</td>\n      <td>10.715176</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>8.532600</td>\n      <td>10.665539</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>8.532600</td>\n      <td>10.644225</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=96, training_loss=23.480180025100708, metrics={'train_runtime': 11.2079, 'train_samples_per_second': 59.958, 'train_steps_per_second': 8.565, 'total_flos': 0.0, 'train_loss': 23.480180025100708, 'epoch': 48.0})"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"# trainer = CustomTrainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     data_collator=custom_data_collator,  # Use the custom collator\n# )\n\n# # Step 7: Test before training\n# try:\n#     # Test that batching works\n#     sample_batch = next(iter(trainer.get_train_dataloader()))\n#     print(\"✓ Batch creation successful\")\n#     print(\"Batch contains:\", list(sample_batch.keys()))\n#     print(\"Labels shape:\", sample_batch['labels'].shape)\n    \n#     # Start training\n#     trainer.train()\n#     trainer.save_model('fine_tuned_hallucination_lora')\n#     print(\"✓ Training completed successfully!\")\n    \n# except Exception as e:\n#     print(f\"✗ Error: {e}\")\n#     # Additional debugging\n#     print(\"Dataset features:\", train_dataset.features)\n#     print(\"First sample:\", {k: type(v) for k, v in train_dataset[0].items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:50:06.360864Z","iopub.execute_input":"2025-09-13T21:50:06.361588Z","iopub.status.idle":"2025-09-13T21:50:06.365316Z","shell.execute_reply.started":"2025-09-13T21:50:06.361561Z","shell.execute_reply":"2025-09-13T21:50:06.364400Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# from transformers import DefaultDataCollator\n\n# from transformers import DataCollatorWithPadding\n# from torch.utils.data import DataLoader\n\n# try:\n#     # data_collator = DefaultDataCollator(return_tensors=\"pt\")\n#     model = HallucinationRegressor(use_lora=True)\n#     trainer = CustomTrainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=train_dataset,\n#         eval_dataset=val_dataset,\n#         data_collator=custom_data_collator,  # Use the custom collator\n#         )\n#     # In training cell, before trainer.train()\n#     print(\"First batch sample:\", next(iter(trainer.get_train_dataloader()))['labels'])\n#     trainer.train()\n#     trainer.save_model('fine_tuned_hallucination_lora')\n#     print(\"Training complete with LoRA. Model saved.\")\n# except Exception as e:\n#     print(f\"Training failed: {e}\")\n#     raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:50:11.031570Z","iopub.execute_input":"2025-09-13T21:50:11.031947Z","iopub.status.idle":"2025-09-13T21:50:11.035431Z","shell.execute_reply.started":"2025-09-13T21:50:11.031924Z","shell.execute_reply":"2025-09-13T21:50:11.034731Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# # Train (enable LoRA here)\n# try:\n#     model = HallucinationRegressor(use_lora=True)  # Add use_lora=True\n#     trainer = CustomTrainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=train_dataset,\n#         eval_dataset=val_dataset,\n#     )\n#     trainer.train()\n#     trainer.save_model('fine_tuned_hallucination_lora')\n#     print(\"Training complete with LoRA. Model saved.\")\n# except Exception as e:\n#     print(f\"Training failed: {e}\")\n#     raise\n\n# # # Train\n# # try:\n# #     model = HallucinationRegressor()\n# #     trainer = CustomTrainer(\n# #         model=model,\n# #         args=training_args,\n# #         train_dataset=train_dataset,\n# #         eval_dataset=val_dataset,\n# #     )\n# #     trainer.train()\n# # except Exception as e:\n# #     print(f\"Training failed: {e}\")\n# #     raise\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:50:16.909762Z","iopub.execute_input":"2025-09-13T21:50:16.910341Z","iopub.status.idle":"2025-09-13T21:50:16.913658Z","shell.execute_reply.started":"2025-09-13T21:50:16.910315Z","shell.execute_reply":"2025-09-13T21:50:16.913100Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# # Save model\n# trainer.save_model('fine_tuned_hallucination')\n# print(\"Training complete. Model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:50:29.113111Z","iopub.execute_input":"2025-09-13T21:50:29.114096Z","iopub.status.idle":"2025-09-13T21:50:29.189534Z","shell.execute_reply.started":"2025-09-13T21:50:29.114054Z","shell.execute_reply":"2025-09-13T21:50:29.188614Z"}},"outputs":[{"name":"stdout","text":"Training complete. Model saved.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Save model\n# import torch\n# import os\n# from transformers import BertTokenizer\n\n# save_path = '/kaggle/working/my_model'\n# os.makedirs(save_path, exist_ok=True)\n\n# torch.save(trainer.model.state_dict(), f'{save_path}/weights.pt')\n# tokenizer = BertTokenizer.from_pretrained('gaunernst/bert-mini-uncased')\n# tokenizer.save_pretrained(f'{save_path}/tokenizer')\n\n# print(\"Model saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:05:26.733734Z","iopub.execute_input":"2025-09-14T06:05:26.734016Z","iopub.status.idle":"2025-09-14T06:05:27.703340Z","shell.execute_reply.started":"2025-09-14T06:05:26.733992Z","shell.execute_reply":"2025-09-14T06:05:27.702129Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/32.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9a166e45e94fbd886fe046144389ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8a724979ee407e9dd0d004f2164fe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a74d96cde6416582149a1b5e88b2f1"}},"metadata":{}},{"name":"stdout","text":"Model saved\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import BertConfig\n\n# Define save path\nsave_path = '/kaggle/working/fine_tuned_model'\nos.makedirs(save_path, exist_ok=True)\n\n# 1. Save full model weights as pytorch_model.bin (includes base, LoRA, regression head)\ntorch.save(trainer.model.state_dict(), os.path.join(save_path, 'pytorch_model.bin'))\nprint(\"✓ Saved pytorch_model.bin\")\n\n# 2. Save base config as config.json\nconfig = BertConfig.from_pretrained('gaunernst/bert-mini-uncased')\nconfig.num_labels = 1  # For regression head\nconfig.save_pretrained(save_path)\nprint(\"✓ Saved config.json\")\n\n# 3. Save tokenizer (generates tokenizer_config.json, special_tokens_map.json, vocab.txt)\ntokenizer.save_pretrained(save_path)\nprint(\"✓ Saved tokenizer files\")\n\nprint(f\"\\nModel saved to {save_path}/\")\nprint(\"Files created:\")\nfor file in sorted(os.listdir(save_path)):\n    print(f\"  - {file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:43:22.923577Z","iopub.execute_input":"2025-09-14T09:43:22.924057Z","iopub.status.idle":"2025-09-14T09:43:23.117448Z","shell.execute_reply.started":"2025-09-14T09:43:22.924023Z","shell.execute_reply":"2025-09-14T09:43:23.116810Z"}},"outputs":[{"name":"stdout","text":"✓ Saved pytorch_model.bin\n✓ Saved config.json\n✓ Saved tokenizer files\n\nModel saved to /kaggle/working/fine_tuned_model/\nFiles created:\n  - config.json\n  - pytorch_model.bin\n  - special_tokens_map.json\n  - tokenizer_config.json\n  - vocab.txt\n","output_type":"stream"}],"execution_count":85},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer\n\nsave_path = '/kaggle/working/my_model'\ntokenizer = BertTokenizer.from_pretrained(f'{save_path}/tokenizer')\nmodel = HallucinationRegressor(use_lora=True)\nmodel.load_state_dict(torch.load(f'{save_path}/weights.pt', map_location='cpu', weights_only=False))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:58:10.123174Z","iopub.execute_input":"2025-09-13T21:58:10.123908Z","iopub.status.idle":"2025-09-13T21:58:10.465868Z","shell.execute_reply.started":"2025-09-13T21:58:10.123883Z","shell.execute_reply":"2025-09-13T21:58:10.465279Z"}},"outputs":[{"name":"stdout","text":"trainable params: 49,152 || all params: 11,219,712 || trainable%: 0.4381\nLoRA enabled with rank 8\nSuccessfully loaded gaunernst/bert-mini-uncased\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"HallucinationRegressor(\n  (base_model): PeftModel(\n    (base_model): LoraModel(\n      (model): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 256, padding_idx=0)\n          (position_embeddings): Embedding(512, 256)\n          (token_type_embeddings): Embedding(2, 256)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-3): 4 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=256, out_features=256, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=256, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=256, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=256, out_features=256, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=256, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=256, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=256, out_features=256, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=256, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=256, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=256, out_features=256, bias=True)\n                  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=256, out_features=1024, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=1024, out_features=256, bias=True)\n                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=256, out_features=256, bias=True)\n          (activation): Tanh()\n        )\n      )\n    )\n  )\n  (regression_head): Linear(in_features=256, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"def load_hallucination_model(save_path='/kaggle/working/fine_tuned_model'):\n    if not os.path.exists(save_path):\n        raise ValueError(f\"Model directory not found: {save_path}\")\n    \n    # Load config\n    config = BertConfig.from_pretrained(save_path)\n    \n    # Initialize model with LoRA (since it was trained with LoRA)\n    model = HallucinationRegressor(use_lora=True)\n    \n    # Load weights\n    state_dict = torch.load(os.path.join(save_path, 'pytorch_model.bin'), map_location='cpu')\n    model.load_state_dict(state_dict)\n    \n    # Set to eval mode\n    model.eval()\n    \n    # Load tokenizer\n    tokenizer = BertTokenizer.from_pretrained(save_path)\n    \n    print(\"Model and tokenizer loaded successfully!\")\n    return model, tokenizer\n\n# Example usage\nmodel, tokenizer = load_hallucination_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:15:09.509621Z","iopub.execute_input":"2025-09-14T06:15:09.510352Z","iopub.status.idle":"2025-09-14T06:15:09.839890Z","shell.execute_reply.started":"2025-09-14T06:15:09.510330Z","shell.execute_reply":"2025-09-14T06:15:09.839272Z"}},"outputs":[{"name":"stdout","text":"trainable params: 49,152 || all params: 11,219,712 || trainable%: 0.4381\nModel and tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"def predict_hallucination(model, tokenizer, prompt, response, jaccard_score):\n    input_text = f\"\"\"We have Response prompt pairs , where prompt is {prompt} and\n    the corresponding response is {response} where the feat 1 is {jaccard_score}\n    give me the hallucination score as output\"\"\"\n    \n    inputs = tokenizer(\n        input_text,\n        return_tensors='pt',\n        truncation=True,\n        padding='max_length',\n        max_length=256\n    )\n    \n    # Remove token_type_ids if your model doesn't need them (BERT-mini might)\n    if 'token_type_ids' in inputs:\n        del inputs['token_type_ids']\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        hallucination_score = outputs['logits'].item()\n    \n    return max(0, min(1, hallucination_score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:15:36.875048Z","iopub.execute_input":"2025-09-14T06:15:36.875597Z","iopub.status.idle":"2025-09-14T06:15:36.880190Z","shell.execute_reply.started":"2025-09-14T06:15:36.875574Z","shell.execute_reply":"2025-09-14T06:15:36.879483Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"prompt = \"What is the capital of France?\"\nresponse = \"The capital is Paris, known for its Eiffel Tower.\"\njaccard = 0.75\nscore = predict_hallucination(model, tokenizer, prompt, response, jaccard)\nprint(f\"Hallucination Score: {score:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T06:15:45.188632Z","iopub.execute_input":"2025-09-14T06:15:45.188890Z","iopub.status.idle":"2025-09-14T06:15:45.264452Z","shell.execute_reply.started":"2025-09-14T06:15:45.188872Z","shell.execute_reply":"2025-09-14T06:15:45.263734Z"}},"outputs":[{"name":"stdout","text":"Hallucination Score: 0.220\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"# def predict_hallucination(prompt, response, jaccard):\n#     data = pd.DataFrame({'prompt': [prompt], 'response': [response], 'jaccard': [jaccard]})\n#     input_string = bert_input_string(0, data)  # Use the function above\n#     inputs = tokenizer(\n#         input_string, \n#         return_tensors='pt', \n#         truncation=True, \n#         padding='max_length', \n#         max_length=256\n#     )\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n#     score = outputs['hallucination_score'].item()\n#     return score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:58:15.036323Z","iopub.execute_input":"2025-09-13T21:58:15.036588Z","iopub.status.idle":"2025-09-13T21:58:15.040555Z","shell.execute_reply.started":"2025-09-13T21:58:15.036570Z","shell.execute_reply":"2025-09-13T21:58:15.039688Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def predict_hallucination(model, tokenizer, prompt, response, jaccard_score):\n    input_text = f\"\"\"We have Response prompt pairs , where prompt is {prompt} and\n    the corresponding response is {response} where the feat 1 is {jaccard_score}\n    give me the hallucination score as output\"\"\"\n    \n    inputs = tokenizer(\n        input_text,\n        return_tensors='pt',\n        truncation=True,\n        padding='max_length',\n        max_length=256\n    )\n    \n    # Remove token_type_ids if present\n    if 'token_type_ids' in inputs:\n        del inputs['token_type_ids']\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        hallucination_score = outputs['logits'].item()\n    \n    return max(0, min(1, hallucination_score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:58:16.095143Z","iopub.execute_input":"2025-09-13T21:58:16.095880Z","iopub.status.idle":"2025-09-13T21:58:16.100841Z","shell.execute_reply.started":"2025-09-13T21:58:16.095855Z","shell.execute_reply":"2025-09-13T21:58:16.099985Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# # Example usage\n# examples = [\n#     (\"What is the capital of France?\", \"The capital is Paris, known for its Eiffel Tower.\", 0.75),\n#     (\"Who won the 2024 Super Bowl?\", \"The Kansas City Chiefs won, beating the Philadelphia Eagles.\", 0.60),\n#     (\"Explain dark matter\", \"Dark matter is invisible stuff that makes up most of the universe and pulls galaxies together with gravity.\", 0.50)\n# ]\n\n# for prompt, response, jaccard in examples:\n#     score = predict_hallucination(prompt, response, jaccard)\n#     print(f\"Prompt: {prompt}\\nResponse: {response}\\nJaccard: {jaccard}\\nHallucination Score: {score:.3f} (0=low, 1=high)\\n---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T21:58:22.279684Z","iopub.execute_input":"2025-09-13T21:58:22.279986Z","iopub.status.idle":"2025-09-13T21:58:22.302266Z","shell.execute_reply.started":"2025-09-13T21:58:22.279966Z","shell.execute_reply":"2025-09-13T21:58:22.301326Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2414858063.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_hallucination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prompt: {prompt}\\nResponse: {response}\\nJaccard: {jaccard}\\nHallucination Score: {score:.3f} (0=low, 1=high)\\n---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: predict_hallucination() missing 2 required positional arguments: 'response' and 'jaccard_score'"],"ename":"TypeError","evalue":"predict_hallucination() missing 2 required positional arguments: 'response' and 'jaccard_score'","output_type":"error"}],"execution_count":58},{"cell_type":"code","source":"\n# Then use the correct function signature\nexamples = [\n    (\"What is the capital of France?\", \"The capital is delhi, known for its taj mahal.\", 0.15),\n    (\"Who won the 2024 Super Bowl?\", \"The Kansas City Chiefs won, beating the Philadelphia Eagles.\", 0.60),\n    (\"Explain dark matter\", \"Dark matter is invisible stuff that makes up most of the universe and pulls galaxies together with gravity.\", 0.30)\n]\n\nif model is not None and tokenizer is not None:\n    for prompt, response, jaccard in examples:\n        score = predict_hallucination(model, tokenizer, prompt, response, jaccard)  # Add model and tokenizer\n        print(f\"Prompt: {prompt}\\nResponse: {response}\\nJaccard: {jaccard}\\nHallucination Score: {score:.3f} (0=low, 1=high)\\n---\")\nelse:\n    print(\"Model or tokenizer failed to load. Check your training completed successfully.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-13T22:29:29.799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}