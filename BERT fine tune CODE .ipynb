{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-14T09:35:51.800619Z",
     "iopub.status.busy": "2025-09-14T09:35:51.800350Z",
     "iopub.status.idle": "2025-09-14T09:35:52.095315Z",
     "shell.execute_reply": "2025-09-14T09:35:52.094596Z",
     "shell.execute_reply.started": "2025-09-14T09:35:51.800587Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/finaldataset/finals.csv\n",
      "/kaggle/input/hallucination-dataset/hallucination_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:35:52.096527Z",
     "iopub.status.busy": "2025-09-14T09:35:52.096112Z",
     "iopub.status.idle": "2025-09-14T09:35:52.102553Z",
     "shell.execute_reply": "2025-09-14T09:35:52.101720Z",
     "shell.execute_reply.started": "2025-09-14T09:35:52.096509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared working cache\n"
     ]
    }
   ],
   "source": [
    "# Cell: Clean Setup and Install (run this first, then RESTART KERNEL)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clear any existing cache to avoid corruption\n",
    "cache_dir = '/root/.cache/huggingface'  # Default HF cache in Kaggle\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(\"Cleared Hugging Face cache\")\n",
    "\n",
    "working_cache = '/kaggle/working/cache'\n",
    "if os.path.exists(working_cache):\n",
    "    shutil.rmtree(working_cache)\n",
    "    print(\"Cleared working cache\")\n",
    "os.makedirs(working_cache, exist_ok=True)\n",
    "\n",
    "# # Install a SINGLE stable version (no conflicts)\n",
    "# !pip uninstall -y transformers tokenizers  # Remove any broken installs first\n",
    "# !pip install transformers==4.41.2 datasets torch scikit-learn pandas numpy\n",
    "\n",
    "# print(\"Installation complete. NOW RESTART YOUR KERNEL (in Kaggle: Run > Restart Session) and continue from imports.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:35:52.104029Z",
     "iopub.status.busy": "2025-09-14T09:35:52.103230Z",
     "iopub.status.idle": "2025-09-14T09:35:55.576758Z",
     "shell.execute_reply": "2025-09-14T09:35:55.575697Z",
     "shell.execute_reply.started": "2025-09-14T09:35:52.104004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.8.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.33.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.5.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.6.15)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft  # Add this if not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:06.563927Z",
     "iopub.status.busy": "2025-09-14T09:36:06.563611Z",
     "iopub.status.idle": "2025-09-14T09:36:34.225530Z",
     "shell.execute_reply": "2025-09-14T09:36:34.224916Z",
     "shell.execute_reply.started": "2025-09-14T09:36:06.563900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 09:36:22.052409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757842582.263514      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757842582.328977      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:03:59.677883Z",
     "iopub.status.busy": "2025-09-14T06:03:59.677590Z",
     "iopub.status.idle": "2025-09-14T06:03:59.825610Z",
     "shell.execute_reply": "2025-09-14T06:03:59.824659Z",
     "shell.execute_reply.started": "2025-09-14T06:03:59.677858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add this cell before dataset creation (cell 25)\n",
    "!rm -rf train_hallucination val_hallucination test_hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:34.227178Z",
     "iopub.status.busy": "2025-09-14T09:36:34.226622Z",
     "iopub.status.idle": "2025-09-14T09:36:36.062959Z",
     "shell.execute_reply": "2025-09-14T09:36:36.062337Z",
     "shell.execute_reply.started": "2025-09-14T09:36:34.227158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:36.063984Z",
     "iopub.status.busy": "2025-09-14T09:36:36.063658Z",
     "iopub.status.idle": "2025-09-14T09:36:36.068352Z",
     "shell.execute_reply": "2025-09-14T09:36:36.067586Z",
     "shell.execute_reply.started": "2025-09-14T09:36:36.063954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device='cuda'if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:36.070793Z",
     "iopub.status.busy": "2025-09-14T09:36:36.070015Z",
     "iopub.status.idle": "2025-09-14T09:36:36.091337Z",
     "shell.execute_reply": "2025-09-14T09:36:36.090502Z",
     "shell.execute_reply.started": "2025-09-14T09:36:36.070774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:36.092355Z",
     "iopub.status.busy": "2025-09-14T09:36:36.092153Z",
     "iopub.status.idle": "2025-09-14T09:36:36.105720Z",
     "shell.execute_reply": "2025-09-14T09:36:36.104996Z",
     "shell.execute_reply.started": "2025-09-14T09:36:36.092332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from transformers import AutoModel, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:55.484442Z",
     "iopub.status.busy": "2025-09-14T09:36:55.483810Z",
     "iopub.status.idle": "2025-09-14T09:36:55.488732Z",
     "shell.execute_reply": "2025-09-14T09:36:55.487902Z",
     "shell.execute_reply.started": "2025-09-14T09:36:55.484417Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.4\n"
     ]
    }
   ],
   "source": [
    "# Verify version\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:56.333211Z",
     "iopub.status.busy": "2025-09-14T09:36:56.332530Z",
     "iopub.status.idle": "2025-09-14T09:36:56.336812Z",
     "shell.execute_reply": "2025-09-14T09:36:56.335981Z",
     "shell.execute_reply.started": "2025-09-14T09:36:56.333185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:57.112281Z",
     "iopub.status.busy": "2025-09-14T09:36:57.111731Z",
     "iopub.status.idle": "2025-09-14T09:36:57.128193Z",
     "shell.execute_reply": "2025-09-14T09:36:57.127562Z",
     "shell.execute_reply.started": "2025-09-14T09:36:57.112256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/finaldataset/finals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:57.858432Z",
     "iopub.status.busy": "2025-09-14T09:36:57.857545Z",
     "iopub.status.idle": "2025-09-14T09:36:57.866240Z",
     "shell.execute_reply": "2025-09-14T09:36:57.865380Z",
     "shell.execute_reply.started": "2025-09-14T09:36:57.858404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'response', 'agent_type', 'Conciseness Score',\n",
       "       'Instruction-Following Score', 'Persona Adherence Score',\n",
       "       'Assumption Control Score', 'user_frustration_proxy', 'overconfidence',\n",
       "       'semantic_alignment', 'semantic_drift', 'repetition_percentage',\n",
       "       'contradiction_detected', 'assumption_risk', 'sentence_variety_score',\n",
       "       'keyword_recall', 'extraneous_ratio', 'alignment_balance',\n",
       "       'spelling_errors', 'number_mismatch_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:36:59.190316Z",
     "iopub.status.busy": "2025-09-14T09:36:59.189588Z",
     "iopub.status.idle": "2025-09-14T09:36:59.200939Z",
     "shell.execute_reply": "2025-09-14T09:36:59.200096Z",
     "shell.execute_reply.started": "2025-09-14T09:36:59.190289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt                         0\n",
       "response                       0\n",
       "agent_type                     0\n",
       "Conciseness Score              0\n",
       "Instruction-Following Score    0\n",
       "Persona Adherence Score        0\n",
       "Assumption Control Score       0\n",
       "user_frustration_proxy         0\n",
       "overconfidence                 0\n",
       "semantic_alignment             0\n",
       "semantic_drift                 0\n",
       "repetition_percentage          0\n",
       "contradiction_detected         0\n",
       "assumption_risk                0\n",
       "sentence_variety_score         0\n",
       "keyword_recall                 0\n",
       "extraneous_ratio               0\n",
       "alignment_balance              0\n",
       "spelling_errors                0\n",
       "number_mismatch_count          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:04:17.157916Z",
     "iopub.status.busy": "2025-09-14T06:04:17.157464Z",
     "iopub.status.idle": "2025-09-14T06:04:17.162369Z",
     "shell.execute_reply": "2025-09-14T06:04:17.161797Z",
     "shell.execute_reply.started": "2025-09-14T06:04:17.157895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:04:17.996491Z",
     "iopub.status.busy": "2025-09-14T06:04:17.996080Z",
     "iopub.status.idle": "2025-09-14T06:04:17.999664Z",
     "shell.execute_reply": "2025-09-14T06:04:17.998834Z",
     "shell.execute_reply.started": "2025-09-14T06:04:17.996471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df=df[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:03.418118Z",
     "iopub.status.busy": "2025-09-14T09:37:03.417391Z",
     "iopub.status.idle": "2025-09-14T09:37:03.424324Z",
     "shell.execute_reply": "2025-09-14T09:37:03.423587Z",
     "shell.execute_reply.started": "2025-09-14T09:37:03.418094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bert_input_string(idx: int, data: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generates a BERT prompt string by fetching all required features from a\n",
    "    single row of a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        idx (int): The index of the row to fetch data from.\n",
    "        data (pd.DataFrame): The DataFrame containing the input features.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt string ready for a BERT model.\n",
    "    \"\"\"\n",
    "    # Fetch the row at the specified index\n",
    "    row = data.iloc[idx]\n",
    "\n",
    "    # Extract all the features from the row\n",
    "# Extract all the features from the row\n",
    "    prompt = row['prompt']\n",
    "    response = row['response']\n",
    "    user_frustration_proxy = row['user_frustration_proxy']\n",
    "    overconfidence = row['overconfidence']\n",
    "    semantic_alignment = row['semantic_alignment']\n",
    "    semantic_drift = row['semantic_drift'] # Not used in prompt\n",
    "    repetition_percentage = row['repetition_percentage']\n",
    "    contradiction_detected = row['contradiction_detected']\n",
    "    assumption_risk = row['assumption_risk']\n",
    "    sentence_variety_score = row['sentence_variety_score'] # Not used in prompt\n",
    "    keyword_recall = row['keyword_recall']\n",
    "    extraneous_ratio = row['extraneous_ratio'] # Not used in prompt\n",
    "    spelling_errors = row['spelling_errors']\n",
    "    \n",
    "    # --- Refined Prompt ---\n",
    "    input_bert = f\"\"\"\n",
    "    [CONTEXT]\n",
    "    Analyze the following user-AI interaction based on the provided text and pre-computed quality metrics.\n",
    "    \n",
    "    [USER PROMPT]\n",
    "    {prompt}\n",
    "    \n",
    "    [AI RESPONSE]\n",
    "    {response}\n",
    "    \n",
    "    [QUALITY METRICS]\n",
    "    - User Frustration Proxy: {user_frustration_proxy:.2%}\n",
    "    - Overconfidence Score: {overconfidence:.2%}\n",
    "    - Semantic Alignment Score: {semantic_alignment:.2%}\n",
    "    - Repetition Percentage: {repetition_percentage:.2%}\n",
    "    - Assumption Risk Score: {assumption_risk:.2%}\n",
    "    - Keyword Recall: {keyword_recall:.2%}\n",
    "    - Spelling Error Count: {spelling_errors}\n",
    "\n",
    "    [TASK]\n",
    "    Based on all the information above, predict the following score:\n",
    "    - Instruction-Following Score\n",
    "    \"\"\" \n",
    "    \n",
    "    return input_bert\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:06.440344Z",
     "iopub.status.busy": "2025-09-14T09:37:06.439712Z",
     "iopub.status.idle": "2025-09-14T09:37:06.450451Z",
     "shell.execute_reply": "2025-09-14T09:37:06.449655Z",
     "shell.execute_reply.started": "2025-09-14T09:37:06.440312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['input_string'] = [bert_input_string(i, df) for i in range(len(df))]\n",
    "df['hallucination_score'] = df['Instruction-Following Score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:08.956167Z",
     "iopub.status.busy": "2025-09-14T09:37:08.955838Z",
     "iopub.status.idle": "2025-09-14T09:37:09.003720Z",
     "shell.execute_reply": "2025-09-14T09:37:09.002990Z",
     "shell.execute_reply.started": "2025-09-14T09:37:08.956146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.1111, random_state=42)\n",
    "train = train.rename(columns={'hallucination_score': 'labels'})\n",
    "val = val.rename(columns={'hallucination_score': 'labels'})\n",
    "test = test.rename(columns={'hallucination_score': 'labels'})\n",
    "train_dataset = Dataset.from_pandas(train[['input_string', 'labels']].reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val[['input_string', 'labels']].reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test[['input_string', 'labels']].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:11.547799Z",
     "iopub.status.busy": "2025-09-14T09:37:11.547488Z",
     "iopub.status.idle": "2025-09-14T09:37:12.510695Z",
     "shell.execute_reply": "2025-09-14T09:37:12.510108Z",
     "shell.execute_reply.started": "2025-09-14T09:37:11.547774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590c3b134a4245f39e34ed17fbedee70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/32.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8126a9ac737e41e389df20f5800d2c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646f66bb5ce247aba041a423998feef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize\n",
    "\n",
    "from transformers import BertTokenizer  # Switch from DistilBertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('gaunernst/bert-mini-uncased', cache_dir='/kaggle/working/cache')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', cache_dir='/kaggle/working/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:15.395941Z",
     "iopub.status.busy": "2025-09-14T09:37:15.395599Z",
     "iopub.status.idle": "2025-09-14T09:37:15.400115Z",
     "shell.execute_reply": "2025-09-14T09:37:15.399300Z",
     "shell.execute_reply.started": "2025-09-14T09:37:15.395907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['input_string'], truncation=True, padding='max_length', max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:19.268831Z",
     "iopub.status.busy": "2025-09-14T09:37:19.268535Z",
     "iopub.status.idle": "2025-09-14T09:37:20.053499Z",
     "shell.execute_reply": "2025-09-14T09:37:20.052585Z",
     "shell.execute_reply.started": "2025-09-14T09:37:19.268807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ca7a771b1746f499effa3390fbfe7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73ef4bb4f104bca9caf2d25ddada53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0a8e0a9cf748bfb39796cb62547d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:22.185078Z",
     "iopub.status.busy": "2025-09-14T09:37:22.184775Z",
     "iopub.status.idle": "2025-09-14T09:37:22.192163Z",
     "shell.execute_reply": "2025-09-14T09:37:22.191388Z",
     "shell.execute_reply.started": "2025-09-14T09:37:22.185056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset features: {'input_string': Value(dtype='string', id=None), 'labels': Value(dtype='float64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "First train sample: {'input_string': \"\\n    [CONTEXT]\\n    Analyze the following user-AI interaction based on the provided text and pre-computed quality metrics.\\n    \\n    [USER PROMPT]\\n    Name the capital of Brazil and the two largest rivers that flow through the country.\\n    \\n    [AI RESPONSE]\\n    Brasília is the capital of Brazil. The Amazon River and the Paraná River are the two largest rivers flowing through the country. The Amazon, the world's largest by discharge volume, traverses the northern region. The Paraná, South America's second-longest river system, flows through or borders the southern parts of the nation.\\n    \\n    [QUALITY METRICS]\\n    - User Frustration Proxy: 69.60%\\n    - Overconfidence Score: 76.30%\\n    - Semantic Alignment Score: 94.90%\\n    - Repetition Percentage: 0.00%\\n    - Assumption Risk Score: 0.00%\\n    - Keyword Recall: 100.00%\\n    - Spelling Error Count: 0\\n\\n    [TASK]\\n    Based on all the information above, predict the following score:\\n    - Instruction-Following Score\\n    \", 'labels': 8.0, 'input_ids': [101, 1031, 6123, 1033, 17908, 1996, 2206, 5310, 1011, 9932, 8290, 2241, 2006, 1996, 3024, 3793, 1998, 3653, 1011, 24806, 3737, 12046, 2015, 1012, 1031, 5310, 25732, 1033, 2171, 1996, 3007, 1997, 4380, 1998, 1996, 2048, 2922, 5485, 2008, 4834, 2083, 1996, 2406, 1012, 1031, 9932, 3433, 1033, 21133, 2401, 2003, 1996, 3007, 1997, 4380, 1012, 1996, 9733, 2314, 1998, 1996, 28383, 2314, 2024, 1996, 2048, 2922, 5485, 8577, 2083, 1996, 2406, 1012, 1996, 9733, 1010, 1996, 2088, 1005, 1055, 2922, 2011, 11889, 3872, 1010, 20811, 2015, 1996, 2642, 2555, 1012, 1996, 28383, 1010, 2148, 2637, 1005, 1055, 2117, 1011, 6493, 2314, 2291, 1010, 6223, 2083, 2030, 6645, 1996, 2670, 3033, 1997, 1996, 3842, 1012, 1031, 3737, 12046, 2015, 1033, 1011, 5310, 9135, 24540, 1024, 6353, 1012, 3438, 1003, 1011, 2058, 8663, 20740, 5897, 3556, 1024, 6146, 1012, 2382, 1003, 1011, 21641, 12139, 3556, 1024, 6365, 1012, 3938, 1003, 1011, 23318, 7017, 1024, 1014, 1012, 4002, 1003, 1011, 11213, 3891, 3556, 1024, 1014, 1012, 4002, 1003, 1011, 3145, 18351, 9131, 1024, 2531, 1012, 4002, 1003, 1011, 11379, 7561, 4175, 1024, 1014, 1031, 4708, 1033, 2241, 2006, 2035, 1996, 2592, 2682, 1010, 16014, 1996, 2206, 3556, 1024, 1011, 7899, 1011, 2206, 3556, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset features:\", train_dataset.features)\n",
    "print(\"First train sample:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:04:30.917856Z",
     "iopub.status.busy": "2025-09-14T06:04:30.917586Z",
     "iopub.status.idle": "2025-09-14T06:04:30.921334Z",
     "shell.execute_reply": "2025-09-14T06:04:30.920587Z",
     "shell.execute_reply.started": "2025-09-14T06:04:30.917835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:04:31.127346Z",
     "iopub.status.busy": "2025-09-14T06:04:31.127167Z",
     "iopub.status.idle": "2025-09-14T06:04:31.130473Z",
     "shell.execute_reply": "2025-09-14T06:04:31.129648Z",
     "shell.execute_reply.started": "2025-09-14T06:04:31.127329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set PyTorch format\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'hallucination_score'])\n",
    "# val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'hallucination_score'])\n",
    "# test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'hallucination_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:28.577061Z",
     "iopub.status.busy": "2025-09-14T09:37:28.576683Z",
     "iopub.status.idle": "2025-09-14T09:37:28.604830Z",
     "shell.execute_reply": "2025-09-14T09:37:28.604161Z",
     "shell.execute_reply.started": "2025-09-14T09:37:28.577037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset after set_format: {'labels': tensor(8.), 'input_ids': tensor([  101,  1031,  6123,  1033, 17908,  1996,  2206,  5310,  1011,  9932,\n",
      "         8290,  2241,  2006,  1996,  3024,  3793,  1998,  3653,  1011, 24806,\n",
      "         3737, 12046,  2015,  1012,  1031,  5310, 25732,  1033,  2171,  1996,\n",
      "         3007,  1997,  4380,  1998,  1996,  2048,  2922,  5485,  2008,  4834,\n",
      "         2083,  1996,  2406,  1012,  1031,  9932,  3433,  1033, 21133,  2401,\n",
      "         2003,  1996,  3007,  1997,  4380,  1012,  1996,  9733,  2314,  1998,\n",
      "         1996, 28383,  2314,  2024,  1996,  2048,  2922,  5485,  8577,  2083,\n",
      "         1996,  2406,  1012,  1996,  9733,  1010,  1996,  2088,  1005,  1055,\n",
      "         2922,  2011, 11889,  3872,  1010, 20811,  2015,  1996,  2642,  2555,\n",
      "         1012,  1996, 28383,  1010,  2148,  2637,  1005,  1055,  2117,  1011,\n",
      "         6493,  2314,  2291,  1010,  6223,  2083,  2030,  6645,  1996,  2670,\n",
      "         3033,  1997,  1996,  3842,  1012,  1031,  3737, 12046,  2015,  1033,\n",
      "         1011,  5310,  9135, 24540,  1024,  6353,  1012,  3438,  1003,  1011,\n",
      "         2058,  8663, 20740,  5897,  3556,  1024,  6146,  1012,  2382,  1003,\n",
      "         1011, 21641, 12139,  3556,  1024,  6365,  1012,  3938,  1003,  1011,\n",
      "        23318,  7017,  1024,  1014,  1012,  4002,  1003,  1011, 11213,  3891,\n",
      "         3556,  1024,  1014,  1012,  4002,  1003,  1011,  3145, 18351,  9131,\n",
      "         1024,  2531,  1012,  4002,  1003,  1011, 11379,  7561,  4175,  1024,\n",
      "         1014,  1031,  4708,  1033,  2241,  2006,  2035,  1996,  2592,  2682,\n",
      "         1010, 16014,  1996,  2206,  3556,  1024,  1011,  7899,  1011,  2206,\n",
      "         3556,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# After tokenization (cell 28)\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "print(\"Train dataset after set_format:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:04:32.807340Z",
     "iopub.status.busy": "2025-09-14T06:04:32.807165Z",
     "iopub.status.idle": "2025-09-14T06:04:32.954366Z",
     "shell.execute_reply": "2025-09-14T06:04:32.953613Z",
     "shell.execute_reply.started": "2025-09-14T06:04:32.807326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf train_hallucination val_hallucination test_hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:33.988484Z",
     "iopub.status.busy": "2025-09-14T09:37:33.988228Z",
     "iopub.status.idle": "2025-09-14T09:37:33.998280Z",
     "shell.execute_reply": "2025-09-14T09:37:33.997495Z",
     "shell.execute_reply.started": "2025-09-14T09:37:33.988468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_string': Value(dtype='string', id=None), 'labels': Value(dtype='float64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "{'labels': tensor(8.), 'input_ids': tensor([  101,  1031,  6123,  1033, 17908,  1996,  2206,  5310,  1011,  9932,\n",
      "         8290,  2241,  2006,  1996,  3024,  3793,  1998,  3653,  1011, 24806,\n",
      "         3737, 12046,  2015,  1012,  1031,  5310, 25732,  1033,  2171,  1996,\n",
      "         3007,  1997,  4380,  1998,  1996,  2048,  2922,  5485,  2008,  4834,\n",
      "         2083,  1996,  2406,  1012,  1031,  9932,  3433,  1033, 21133,  2401,\n",
      "         2003,  1996,  3007,  1997,  4380,  1012,  1996,  9733,  2314,  1998,\n",
      "         1996, 28383,  2314,  2024,  1996,  2048,  2922,  5485,  8577,  2083,\n",
      "         1996,  2406,  1012,  1996,  9733,  1010,  1996,  2088,  1005,  1055,\n",
      "         2922,  2011, 11889,  3872,  1010, 20811,  2015,  1996,  2642,  2555,\n",
      "         1012,  1996, 28383,  1010,  2148,  2637,  1005,  1055,  2117,  1011,\n",
      "         6493,  2314,  2291,  1010,  6223,  2083,  2030,  6645,  1996,  2670,\n",
      "         3033,  1997,  1996,  3842,  1012,  1031,  3737, 12046,  2015,  1033,\n",
      "         1011,  5310,  9135, 24540,  1024,  6353,  1012,  3438,  1003,  1011,\n",
      "         2058,  8663, 20740,  5897,  3556,  1024,  6146,  1012,  2382,  1003,\n",
      "         1011, 21641, 12139,  3556,  1024,  6365,  1012,  3938,  1003,  1011,\n",
      "        23318,  7017,  1024,  1014,  1012,  4002,  1003,  1011, 11213,  3891,\n",
      "         3556,  1024,  1014,  1012,  4002,  1003,  1011,  3145, 18351,  9131,\n",
      "         1024,  2531,  1012,  4002,  1003,  1011, 11379,  7561,  4175,  1024,\n",
      "         1014,  1031,  4708,  1033,  2241,  2006,  2035,  1996,  2592,  2682,\n",
      "         1010, 16014,  1996,  2206,  3556,  1024,  1011,  7899,  1011,  2206,\n",
      "         3556,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features)  # Should show 'labels': FloatField or similar\n",
    "print(train_dataset[0])  # Should include 'labels' as a tensor or value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:37.036068Z",
     "iopub.status.busy": "2025-09-14T09:37:37.035377Z",
     "iopub.status.idle": "2025-09-14T09:37:37.108473Z",
     "shell.execute_reply": "2025-09-14T09:37:37.107728Z",
     "shell.execute_reply.started": "2025-09-14T09:37:37.036042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456c3e04052c4ff39d21611227d852cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9f017c5c334695b051fa0ffc8c0e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a770c796ae548c1baf574874bcc5fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Datasets saved.\n"
     ]
    }
   ],
   "source": [
    "# Save datasets\n",
    "train_dataset.save_to_disk('train_hallucination')\n",
    "val_dataset.save_to_disk('val_hallucination')\n",
    "test_dataset.save_to_disk('test_hallucination')\n",
    "\n",
    "print(\"Preprocessing complete. Datasets saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.56.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import transformers\n",
    "# transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T20:52:53.599882Z",
     "iopub.status.busy": "2025-09-13T20:52:53.599589Z",
     "iopub.status.idle": "2025-09-13T20:52:53.609488Z",
     "shell.execute_reply": "2025-09-13T20:52:53.608800Z",
     "shell.execute_reply.started": "2025-09-13T20:52:53.599854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import BertModel, BertTokenizer  # Switch to Bert* for bert-mini-uncased (explained in section 2)\n",
    "# from peft import LoraConfig, get_peft_model  # For LoRA\n",
    "\n",
    "# class HallucinationRegressor(nn.Module):\n",
    "#     def __init__(self, base_model_name='gaunernst/bert-mini-uncased', use_lora=False, lora_rank=8):\n",
    "#         super().__init__()\n",
    "#         try:\n",
    "#             self.base_model = BertModel.from_pretrained(\n",
    "#                 base_model_name,\n",
    "#                 cache_dir='/kaggle/working/cache'\n",
    "#             )\n",
    "#             if use_lora:\n",
    "#                 # Configure LoRA (target attention layers for efficiency)\n",
    "#                 peft_config = LoraConfig(\n",
    "#                     r=lora_rank,  # Low-rank dimension (8-32 typical; higher=better quality, more params)\n",
    "#                     lora_alpha=16,  # Scaling factor\n",
    "#                     lora_dropout=0.1,\n",
    "#                     bias=\"none\",\n",
    "#                     target_modules=[\"query\", \"key\", \"value\"]  # Focus on attention for BERT-like models\n",
    "#                 )\n",
    "#                 self.base_model = get_peft_model(self.base_model, peft_config)\n",
    "#                 self.base_model.print_trainable_parameters()  # Logs % trainable params (should be low)\n",
    "#                 print(f\"LoRA enabled with rank {lora_rank}\")\n",
    "#             print(f\"Successfully loaded {base_model_name}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to load model: {e}\")\n",
    "#             raise\n",
    "#         # Regression head on pooled output\n",
    "#         self.regression_head = nn.Linear(self.base_model.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token pooling\n",
    "#         hallucination_score = self.regression_head(pooled_output).squeeze(-1)\n",
    "#         return {'hallucination_score': hallucination_score}\n",
    "\n",
    "# # class HallucinationRegressor(nn.Module):\n",
    "# #     def __init__(self, base_model_name='distilbert-base-uncased'):\n",
    "# #         super().__init__()\n",
    "# #         try:\n",
    "# #             # Load directly with DistilBertModel to bypass AutoModel issues\n",
    "# #             self.bert = DistilBertModel.from_pretrained(\n",
    "# #                 base_model_name,\n",
    "# #                 cache_dir='/kaggle/working/cache'  # Use your cleared working cache\n",
    "# #             )\n",
    "# #             print(f\"Successfully loaded {base_model_name}\")\n",
    "# #         except Exception as e:\n",
    "# #             print(f\"Failed to load model: {e}\")\n",
    "# #             raise\n",
    "# #         self.regression_head = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "# #     def forward(self, input_ids, attention_mask):\n",
    "# #         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "# #         # Use [CLS] token for pooling (DistilBERT uses last_hidden_state[:, 0, :])\n",
    "# #         pooled_output = outputs.last_hidden_state[:, 0]\n",
    "# #         hallucination_score = self.regression_head(pooled_output).squeeze(-1)\n",
    "# #         return {'hallucination_score': hallucination_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:41.145912Z",
     "iopub.status.busy": "2025-09-14T09:37:41.145582Z",
     "iopub.status.idle": "2025-09-14T09:37:41.182270Z",
     "shell.execute_reply": "2025-09-14T09:37:41.181695Z",
     "shell.execute_reply.started": "2025-09-14T09:37:41.145857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# FIXED VERSION - Remove the extra 'self' parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer  # Switch to Bert* for bert-mini-uncased (explained in section 2)\n",
    "from peft import LoraConfig, get_peft_model  # For LoRA\n",
    "class HallucinationRegressor(nn.Module):\n",
    "    def __init__(self, base_model_name='gaunernst/bert-mini-uncased', use_lora=False, lora_rank=8):\n",
    "        super().__init__()  # Remove the 'self' parameter here\n",
    "        try:\n",
    "            self.base_model = BertModel.from_pretrained(\n",
    "                base_model_name,\n",
    "                cache_dir='/kaggle/working/cache'\n",
    "            )\n",
    "            if use_lora:\n",
    "                peft_config = LoraConfig(\n",
    "                    r=lora_rank,\n",
    "                    lora_alpha=16,\n",
    "                    lora_dropout=0.1,\n",
    "                    bias=\"none\",\n",
    "                    target_modules=[\"query\", \"key\", \"value\"]\n",
    "                )\n",
    "                self.base_model = get_peft_model(self.base_model, peft_config)\n",
    "                self.base_model.print_trainable_parameters()\n",
    "                print(f\"LoRA enabled with rank {lora_rank}\")\n",
    "            print(f\"Successfully loaded {base_model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "        self.regression_head = nn.Linear(self.base_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.regression_head(pooled_output).squeeze(-1)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.MSELoss()(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Now this should work:\n",
    "# model = HallucinationRegressor(use_lora=True)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:44.297285Z",
     "iopub.status.busy": "2025-09-14T09:37:44.296651Z",
     "iopub.status.idle": "2025-09-14T09:37:44.304476Z",
     "shell.execute_reply": "2025-09-14T09:37:44.303657Z",
     "shell.execute_reply.started": "2025-09-14T09:37:44.297258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType  # Ensure TaskType is imported\n",
    "\n",
    "class HallucinationRegressor(nn.Module):\n",
    "    def __init__(self, use_lora=False, lora_rank=8):\n",
    "        super().__init__()\n",
    "        self.base_model = BertModel.from_pretrained('gaunernst/bert-mini-uncased')\n",
    "        self.regression_head = nn.Linear(self.base_model.config.hidden_size, 1)\n",
    "        \n",
    "        if use_lora:\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.FEATURE_EXTRACTION,  # <-- CHANGE HERE: Use FEATURE_EXTRACTION for encoder-only\n",
    "                inference_mode=False,\n",
    "                r=lora_rank,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"query\", \"value\", \"key\"]\n",
    "            )\n",
    "            self.base_model = get_peft_model(self.base_model, lora_config)\n",
    "            self.base_model.print_trainable_parameters()\n",
    "        \n",
    "        # Freeze base unless LoRA (already handled by PEFT)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        if token_type_ids is not None:\n",
    "            del token_type_ids  # Avoid passing unused token_type_ids to base_model\n",
    "        \n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.regression_head(pooled_output)\n",
    "        \n",
    "        # Compute loss only if labels provided (for Trainer compatibility)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.MSELoss()(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:47.242217Z",
     "iopub.status.busy": "2025-09-14T09:37:47.241454Z",
     "iopub.status.idle": "2025-09-14T09:37:47.253360Z",
     "shell.execute_reply": "2025-09-14T09:37:47.252687Z",
     "shell.execute_reply.started": "2025-09-14T09:37:47.242191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = load_from_disk('train_hallucination')\n",
    "val_dataset = load_from_disk('val_hallucination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:49.304193Z",
     "iopub.status.busy": "2025-09-14T09:37:49.303597Z",
     "iopub.status.idle": "2025-09-14T09:37:49.309590Z",
     "shell.execute_reply": "2025-09-14T09:37:49.308923Z",
     "shell.execute_reply.started": "2025-09-14T09:37:49.304170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "val_dataset = val_dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:09:46.787759Z",
     "iopub.status.busy": "2025-09-14T06:09:46.787470Z",
     "iopub.status.idle": "2025-09-14T06:09:46.797091Z",
     "shell.execute_reply": "2025-09-14T06:09:46.796430Z",
     "shell.execute_reply.started": "2025-09-14T06:09:46.787736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_string': Value(dtype='string', id=None), 'labels': Value(dtype='float64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "{'input_string': 'We have Response prompt pairs , where prompt is Explain quantum mechanics simply and\\n    the corresponding response is Quantum mechanics studies how particles like electrons behave at tiny scales, often acting like waves and particles simultaneously. where the feat 1 is 0.45\\n    give me the hallucination score as output', 'labels': tensor(0.1000), 'input_ids': tensor([  101,  2057,  2031,  3433, 25732,  7689,  1010,  2073, 25732,  2003,\n",
      "         4863,  8559,  9760,  3432,  1998,  1996,  7978,  3433,  2003,  8559,\n",
      "         9760,  2913,  2129,  9309,  2066, 15057, 16582,  2012,  4714,  9539,\n",
      "         1010,  2411,  3772,  2066,  5975,  1998,  9309,  7453,  1012,  2073,\n",
      "         1996,  8658,  1015,  2003,  1014,  1012,  3429,  2507,  2033,  1996,\n",
      "         2534, 14194, 12758,  3556,  2004,  6434,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features)  # Should show 'labels': FloatField or similar\n",
    "print(train_dataset[0])  # Should include 'labels' as a tensor or value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:01:47.621432Z",
     "iopub.status.busy": "2025-09-13T21:01:47.621170Z",
     "iopub.status.idle": "2025-09-13T21:01:47.625993Z",
     "shell.execute_reply": "2025-09-13T21:01:47.625108Z",
     "shell.execute_reply.started": "2025-09-13T21:01:47.621414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# def compute_loss(model, inputs, return_outputs=False):\n",
    "#     labels = inputs.pop('hallucination_score').to(model.device)\n",
    "#     outputs = model(**inputs)\n",
    "#     loss = nn.MSELoss()(outputs['hallucination_score'], labels)\n",
    "#     return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:11:57.717170Z",
     "iopub.status.busy": "2025-09-14T06:11:57.716488Z",
     "iopub.status.idle": "2025-09-14T06:11:57.721170Z",
     "shell.execute_reply": "2025-09-14T06:11:57.720450Z",
     "shell.execute_reply.started": "2025-09-14T06:11:57.717145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def compute_loss(model, inputs, return_outputs=False):\n",
    "#     print(\"Inputs to compute_loss:\", inputs.keys())  # Debug\n",
    "#     labels = inputs.pop('labels').to(model.device)\n",
    "#     outputs = model(**inputs)\n",
    "#     loss = nn.MSELoss()(outputs['hallucination_score'], labels)\n",
    "#     return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:53.497763Z",
     "iopub.status.busy": "2025-09-14T09:37:53.497508Z",
     "iopub.status.idle": "2025-09-14T09:37:53.502655Z",
     "shell.execute_reply": "2025-09-14T09:37:53.501730Z",
     "shell.execute_reply.started": "2025-09-14T09:37:53.497745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Update Cell 34 to use 'logits' (not 'hallucination_score')\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    labels = inputs.pop('labels').to(model.device)\n",
    "    outputs = model(**inputs)\n",
    "    loss = nn.MSELoss()(outputs['logits'], labels)  # Fix key to 'logits'\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:42:53.906039Z",
     "iopub.status.busy": "2025-09-14T09:42:53.905407Z",
     "iopub.status.idle": "2025-09-14T09:42:53.944577Z",
     "shell.execute_reply": "2025-09-14T09:42:53.943788Z",
     "shell.execute_reply.started": "2025-09-14T09:42:53.906008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/hallucination_results',\n",
    "    num_train_epochs=48,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    warmup_steps=10,\n",
    "    gradient_accumulation_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/kaggle/working/hallucination_logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:42:54.715838Z",
     "iopub.status.busy": "2025-09-14T09:42:54.715562Z",
     "iopub.status.idle": "2025-09-14T09:42:54.720750Z",
     "shell.execute_reply": "2025-09-14T09:42:54.719804Z",
     "shell.execute_reply.started": "2025-09-14T09:42:54.715818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def custom_data_collator(features):\n",
    "    \"\"\"Custom collator that preserves labels\"\"\"\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack([f['input_ids'] for f in features])\n",
    "    batch['attention_mask'] = torch.stack([f['attention_mask'] for f in features])\n",
    "    batch['labels'] = torch.stack([f['labels'] for f in features])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:42:55.481751Z",
     "iopub.status.busy": "2025-09-14T09:42:55.481463Z",
     "iopub.status.idle": "2025-09-14T09:42:55.485455Z",
     "shell.execute_reply": "2025-09-14T09:42:55.484734Z",
     "shell.execute_reply.started": "2025-09-14T09:42:55.481730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Custom Trainer\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         return compute_loss(model, inputs, return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:42:56.640841Z",
     "iopub.status.busy": "2025-09-14T09:42:56.640559Z",
     "iopub.status.idle": "2025-09-14T09:42:56.644727Z",
     "shell.execute_reply": "2025-09-14T09:42:56.643919Z",
     "shell.execute_reply.started": "2025-09-14T09:42:56.640821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "#         return compute_loss(model, inputs, return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:42:57.981385Z",
     "iopub.status.busy": "2025-09-14T09:42:57.980645Z",
     "iopub.status.idle": "2025-09-14T09:42:57.984326Z",
     "shell.execute_reply": "2025-09-14T09:42:57.983644Z",
     "shell.execute_reply.started": "2025-09-14T09:42:57.981358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # print(\"Dataset sample:\", train_dataset[0])\n",
    "# print(\"Dataset columns after set_format:\", list(train_dataset[0].keys()))\n",
    "# sample_batch = data_collator([train_dataset[0], train_dataset[1]])\n",
    "# print(\"Collated batch keys:\", list(sample_batch.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:42:58.542064Z",
     "iopub.status.busy": "2025-09-14T09:42:58.541598Z",
     "iopub.status.idle": "2025-09-14T09:42:58.545830Z",
     "shell.execute_reply": "2025-09-14T09:42:58.545178Z",
     "shell.execute_reply.started": "2025-09-14T09:42:58.542041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:42:59.243372Z",
     "iopub.status.busy": "2025-09-14T09:42:59.242761Z",
     "iopub.status.idle": "2025-09-14T09:43:11.250572Z",
     "shell.execute_reply": "2025-09-14T09:43:11.249999Z",
     "shell.execute_reply.started": "2025-09-14T09:42:59.243345Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 49,152 || all params: 11,219,712 || trainable%: 0.4381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:11, Epoch 48/48]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>82.789543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>81.451683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>78.930336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>75.018799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>61.667200</td>\n",
       "      <td>69.354630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>61.667200</td>\n",
       "      <td>62.138542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>61.667200</td>\n",
       "      <td>55.649681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>61.667200</td>\n",
       "      <td>51.059120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>61.667200</td>\n",
       "      <td>47.771805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>45.530500</td>\n",
       "      <td>44.947052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>45.530500</td>\n",
       "      <td>42.354870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>45.530500</td>\n",
       "      <td>40.057014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>45.530500</td>\n",
       "      <td>37.908203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>45.530500</td>\n",
       "      <td>35.778969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>30.766500</td>\n",
       "      <td>33.732391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>30.766500</td>\n",
       "      <td>31.813366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>30.766500</td>\n",
       "      <td>30.069111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>30.766500</td>\n",
       "      <td>28.511688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>30.766500</td>\n",
       "      <td>27.021877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>22.220400</td>\n",
       "      <td>25.637085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>22.220400</td>\n",
       "      <td>24.338785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22.220400</td>\n",
       "      <td>23.107121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>22.220400</td>\n",
       "      <td>21.992249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>22.220400</td>\n",
       "      <td>20.986250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>16.961200</td>\n",
       "      <td>20.032547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>16.961200</td>\n",
       "      <td>19.136059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>16.961200</td>\n",
       "      <td>18.274111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>16.961200</td>\n",
       "      <td>17.461922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>16.961200</td>\n",
       "      <td>16.715481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>13.597500</td>\n",
       "      <td>16.038979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>13.597500</td>\n",
       "      <td>15.416554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>13.597500</td>\n",
       "      <td>14.839019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>13.597500</td>\n",
       "      <td>14.310478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>13.597500</td>\n",
       "      <td>13.823582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>10.916900</td>\n",
       "      <td>13.367225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>10.916900</td>\n",
       "      <td>12.942493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>10.916900</td>\n",
       "      <td>12.555920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>10.916900</td>\n",
       "      <td>12.210182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>10.916900</td>\n",
       "      <td>11.906526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>10.019700</td>\n",
       "      <td>11.640856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>10.019700</td>\n",
       "      <td>11.408796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>10.019700</td>\n",
       "      <td>11.206953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>10.019700</td>\n",
       "      <td>11.038471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>10.019700</td>\n",
       "      <td>10.901005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>8.532600</td>\n",
       "      <td>10.793736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>8.532600</td>\n",
       "      <td>10.715176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>8.532600</td>\n",
       "      <td>10.665539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>8.532600</td>\n",
       "      <td>10.644225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=23.480180025100708, metrics={'train_runtime': 11.2079, 'train_samples_per_second': 59.958, 'train_steps_per_second': 8.565, 'total_flos': 0.0, 'train_loss': 23.480180025100708, 'epoch': 48.0})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HallucinationRegressor(use_lora=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:50:06.361588Z",
     "iopub.status.busy": "2025-09-13T21:50:06.360864Z",
     "iopub.status.idle": "2025-09-13T21:50:06.365316Z",
     "shell.execute_reply": "2025-09-13T21:50:06.364400Z",
     "shell.execute_reply.started": "2025-09-13T21:50:06.361561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# trainer = CustomTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     data_collator=custom_data_collator,  # Use the custom collator\n",
    "# )\n",
    "\n",
    "# # Step 7: Test before training\n",
    "# try:\n",
    "#     # Test that batching works\n",
    "#     sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "#     print(\"✓ Batch creation successful\")\n",
    "#     print(\"Batch contains:\", list(sample_batch.keys()))\n",
    "#     print(\"Labels shape:\", sample_batch['labels'].shape)\n",
    "    \n",
    "#     # Start training\n",
    "#     trainer.train()\n",
    "#     trainer.save_model('fine_tuned_hallucination_lora')\n",
    "#     print(\"✓ Training completed successfully!\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"✗ Error: {e}\")\n",
    "#     # Additional debugging\n",
    "#     print(\"Dataset features:\", train_dataset.features)\n",
    "#     print(\"First sample:\", {k: type(v) for k, v in train_dataset[0].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:50:11.031947Z",
     "iopub.status.busy": "2025-09-13T21:50:11.031570Z",
     "iopub.status.idle": "2025-09-13T21:50:11.035431Z",
     "shell.execute_reply": "2025-09-13T21:50:11.034731Z",
     "shell.execute_reply.started": "2025-09-13T21:50:11.031924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import DefaultDataCollator\n",
    "\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# try:\n",
    "#     # data_collator = DefaultDataCollator(return_tensors=\"pt\")\n",
    "#     model = HallucinationRegressor(use_lora=True)\n",
    "#     trainer = CustomTrainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=val_dataset,\n",
    "#         data_collator=custom_data_collator,  # Use the custom collator\n",
    "#         )\n",
    "#     # In training cell, before trainer.train()\n",
    "#     print(\"First batch sample:\", next(iter(trainer.get_train_dataloader()))['labels'])\n",
    "#     trainer.train()\n",
    "#     trainer.save_model('fine_tuned_hallucination_lora')\n",
    "#     print(\"Training complete with LoRA. Model saved.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Training failed: {e}\")\n",
    "#     raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:50:16.910341Z",
     "iopub.status.busy": "2025-09-13T21:50:16.909762Z",
     "iopub.status.idle": "2025-09-13T21:50:16.913658Z",
     "shell.execute_reply": "2025-09-13T21:50:16.913100Z",
     "shell.execute_reply.started": "2025-09-13T21:50:16.910315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Train (enable LoRA here)\n",
    "# try:\n",
    "#     model = HallucinationRegressor(use_lora=True)  # Add use_lora=True\n",
    "#     trainer = CustomTrainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=val_dataset,\n",
    "#     )\n",
    "#     trainer.train()\n",
    "#     trainer.save_model('fine_tuned_hallucination_lora')\n",
    "#     print(\"Training complete with LoRA. Model saved.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Training failed: {e}\")\n",
    "#     raise\n",
    "\n",
    "# # # Train\n",
    "# # try:\n",
    "# #     model = HallucinationRegressor()\n",
    "# #     trainer = CustomTrainer(\n",
    "# #         model=model,\n",
    "# #         args=training_args,\n",
    "# #         train_dataset=train_dataset,\n",
    "# #         eval_dataset=val_dataset,\n",
    "# #     )\n",
    "# #     trainer.train()\n",
    "# # except Exception as e:\n",
    "# #     print(f\"Training failed: {e}\")\n",
    "# #     raise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:50:29.114096Z",
     "iopub.status.busy": "2025-09-13T21:50:29.113111Z",
     "iopub.status.idle": "2025-09-13T21:50:29.189534Z",
     "shell.execute_reply": "2025-09-13T21:50:29.188614Z",
     "shell.execute_reply.started": "2025-09-13T21:50:29.114054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "# # Save model\n",
    "# trainer.save_model('fine_tuned_hallucination')\n",
    "# print(\"Training complete. Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:05:26.734016Z",
     "iopub.status.busy": "2025-09-14T06:05:26.733734Z",
     "iopub.status.idle": "2025-09-14T06:05:27.703340Z",
     "shell.execute_reply": "2025-09-14T06:05:27.702129Z",
     "shell.execute_reply.started": "2025-09-14T06:05:26.733992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9a166e45e94fbd886fe046144389ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/32.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8a724979ee407e9dd0d004f2164fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a74d96cde6416582149a1b5e88b2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "# import torch\n",
    "# import os\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# save_path = '/kaggle/working/my_model'\n",
    "# os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# torch.save(trainer.model.state_dict(), f'{save_path}/weights.pt')\n",
    "# tokenizer = BertTokenizer.from_pretrained('gaunernst/bert-mini-uncased')\n",
    "# tokenizer.save_pretrained(f'{save_path}/tokenizer')\n",
    "\n",
    "# print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:43:22.924057Z",
     "iopub.status.busy": "2025-09-14T09:43:22.923577Z",
     "iopub.status.idle": "2025-09-14T09:43:23.117448Z",
     "shell.execute_reply": "2025-09-14T09:43:23.116810Z",
     "shell.execute_reply.started": "2025-09-14T09:43:22.924023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved pytorch_model.bin\n",
      "✓ Saved config.json\n",
      "✓ Saved tokenizer files\n",
      "\n",
      "Model saved to /kaggle/working/fine_tuned_model/\n",
      "Files created:\n",
      "  - config.json\n",
      "  - pytorch_model.bin\n",
      "  - special_tokens_map.json\n",
      "  - tokenizer_config.json\n",
      "  - vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertConfig\n",
    "\n",
    "# Define save path\n",
    "save_path = '/kaggle/working/fine_tuned_model'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# 1. Save full model weights as pytorch_model.bin (includes base, LoRA, regression head)\n",
    "torch.save(trainer.model.state_dict(), os.path.join(save_path, 'pytorch_model.bin'))\n",
    "print(\"✓ Saved pytorch_model.bin\")\n",
    "\n",
    "# 2. Save base config as config.json\n",
    "config = BertConfig.from_pretrained('gaunernst/bert-mini-uncased')\n",
    "config.num_labels = 1  # For regression head\n",
    "config.save_pretrained(save_path)\n",
    "print(\"✓ Saved config.json\")\n",
    "\n",
    "# 3. Save tokenizer (generates tokenizer_config.json, special_tokens_map.json, vocab.txt)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"✓ Saved tokenizer files\")\n",
    "\n",
    "print(f\"\\nModel saved to {save_path}/\")\n",
    "print(\"Files created:\")\n",
    "for file in sorted(os.listdir(save_path)):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:58:10.123908Z",
     "iopub.status.busy": "2025-09-13T21:58:10.123174Z",
     "iopub.status.idle": "2025-09-13T21:58:10.465868Z",
     "shell.execute_reply": "2025-09-13T21:58:10.465279Z",
     "shell.execute_reply.started": "2025-09-13T21:58:10.123883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 49,152 || all params: 11,219,712 || trainable%: 0.4381\n",
      "LoRA enabled with rank 8\n",
      "Successfully loaded gaunernst/bert-mini-uncased\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HallucinationRegressor(\n",
       "  (base_model): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 256)\n",
       "          (token_type_embeddings): Embedding(2, 256)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-3): 4 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (regression_head): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "save_path = '/kaggle/working/my_model'\n",
    "tokenizer = BertTokenizer.from_pretrained(f'{save_path}/tokenizer')\n",
    "model = HallucinationRegressor(use_lora=True)\n",
    "model.load_state_dict(torch.load(f'{save_path}/weights.pt', map_location='cpu', weights_only=False))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:15:09.510352Z",
     "iopub.status.busy": "2025-09-14T06:15:09.509621Z",
     "iopub.status.idle": "2025-09-14T06:15:09.839890Z",
     "shell.execute_reply": "2025-09-14T06:15:09.839272Z",
     "shell.execute_reply.started": "2025-09-14T06:15:09.510330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 49,152 || all params: 11,219,712 || trainable%: 0.4381\n",
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_hallucination_model(save_path='/kaggle/working/fine_tuned_model'):\n",
    "    if not os.path.exists(save_path):\n",
    "        raise ValueError(f\"Model directory not found: {save_path}\")\n",
    "    \n",
    "    # Load config\n",
    "    config = BertConfig.from_pretrained(save_path)\n",
    "    \n",
    "    # Initialize model with LoRA (since it was trained with LoRA)\n",
    "    model = HallucinationRegressor(use_lora=True)\n",
    "    \n",
    "    # Load weights\n",
    "    state_dict = torch.load(os.path.join(save_path, 'pytorch_model.bin'), map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Set to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(save_path)\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Example usage\n",
    "model, tokenizer = load_hallucination_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:15:36.875597Z",
     "iopub.status.busy": "2025-09-14T06:15:36.875048Z",
     "iopub.status.idle": "2025-09-14T06:15:36.880190Z",
     "shell.execute_reply": "2025-09-14T06:15:36.879483Z",
     "shell.execute_reply.started": "2025-09-14T06:15:36.875574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_hallucination(model, tokenizer, prompt, response, jaccard_score):\n",
    "    input_text = f\"\"\"We have Response prompt pairs , where prompt is {prompt} and\n",
    "    the corresponding response is {response} where the feat 1 is {jaccard_score}\n",
    "    give me the hallucination score as output\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # Remove token_type_ids if your model doesn't need them (BERT-mini might)\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hallucination_score = outputs['logits'].item()\n",
    "    \n",
    "    return max(0, min(1, hallucination_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T06:15:45.188890Z",
     "iopub.status.busy": "2025-09-14T06:15:45.188632Z",
     "iopub.status.idle": "2025-09-14T06:15:45.264452Z",
     "shell.execute_reply": "2025-09-14T06:15:45.263734Z",
     "shell.execute_reply.started": "2025-09-14T06:15:45.188872Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination Score: 0.220\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "response = \"The capital is Paris, known for its Eiffel Tower.\"\n",
    "jaccard = 0.75\n",
    "score = predict_hallucination(model, tokenizer, prompt, response, jaccard)\n",
    "print(f\"Hallucination Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:58:15.036588Z",
     "iopub.status.busy": "2025-09-13T21:58:15.036323Z",
     "iopub.status.idle": "2025-09-13T21:58:15.040555Z",
     "shell.execute_reply": "2025-09-13T21:58:15.039688Z",
     "shell.execute_reply.started": "2025-09-13T21:58:15.036570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def predict_hallucination(prompt, response, jaccard):\n",
    "#     data = pd.DataFrame({'prompt': [prompt], 'response': [response], 'jaccard': [jaccard]})\n",
    "#     input_string = bert_input_string(0, data)  # Use the function above\n",
    "#     inputs = tokenizer(\n",
    "#         input_string, \n",
    "#         return_tensors='pt', \n",
    "#         truncation=True, \n",
    "#         padding='max_length', \n",
    "#         max_length=256\n",
    "#     )\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     score = outputs['hallucination_score'].item()\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:58:16.095880Z",
     "iopub.status.busy": "2025-09-13T21:58:16.095143Z",
     "iopub.status.idle": "2025-09-13T21:58:16.100841Z",
     "shell.execute_reply": "2025-09-13T21:58:16.099985Z",
     "shell.execute_reply.started": "2025-09-13T21:58:16.095855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_hallucination(model, tokenizer, prompt, response, jaccard_score):\n",
    "    input_text = f\"\"\"We have Response prompt pairs , where prompt is {prompt} and\n",
    "    the corresponding response is {response} where the feat 1 is {jaccard_score}\n",
    "    give me the hallucination score as output\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    # Remove token_type_ids if present\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hallucination_score = outputs['logits'].item()\n",
    "    \n",
    "    return max(0, min(1, hallucination_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T21:58:22.279986Z",
     "iopub.status.busy": "2025-09-13T21:58:22.279684Z",
     "iopub.status.idle": "2025-09-13T21:58:22.302266Z",
     "shell.execute_reply": "2025-09-13T21:58:22.301326Z",
     "shell.execute_reply.started": "2025-09-13T21:58:22.279966Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_hallucination() missing 2 required positional arguments: 'response' and 'jaccard_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2414858063.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_hallucination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaccard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prompt: {prompt}\\nResponse: {response}\\nJaccard: {jaccard}\\nHallucination Score: {score:.3f} (0=low, 1=high)\\n---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_hallucination() missing 2 required positional arguments: 'response' and 'jaccard_score'"
     ]
    }
   ],
   "source": [
    "# # Example usage\n",
    "# examples = [\n",
    "#     (\"What is the capital of France?\", \"The capital is Paris, known for its Eiffel Tower.\", 0.75),\n",
    "#     (\"Who won the 2024 Super Bowl?\", \"The Kansas City Chiefs won, beating the Philadelphia Eagles.\", 0.60),\n",
    "#     (\"Explain dark matter\", \"Dark matter is invisible stuff that makes up most of the universe and pulls galaxies together with gravity.\", 0.50)\n",
    "# ]\n",
    "\n",
    "# for prompt, response, jaccard in examples:\n",
    "#     score = predict_hallucination(prompt, response, jaccard)\n",
    "#     print(f\"Prompt: {prompt}\\nResponse: {response}\\nJaccard: {jaccard}\\nHallucination Score: {score:.3f} (0=low, 1=high)\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-13T22:29:29.799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Then use the correct function signature\n",
    "examples = [\n",
    "    (\"What is the capital of France?\", \"The capital is delhi, known for its taj mahal.\", 0.15),\n",
    "    (\"Who won the 2024 Super Bowl?\", \"The Kansas City Chiefs won, beating the Philadelphia Eagles.\", 0.60),\n",
    "    (\"Explain dark matter\", \"Dark matter is invisible stuff that makes up most of the universe and pulls galaxies together with gravity.\", 0.30)\n",
    "]\n",
    "\n",
    "if model is not None and tokenizer is not None:\n",
    "    for prompt, response, jaccard in examples:\n",
    "        score = predict_hallucination(model, tokenizer, prompt, response, jaccard)  # Add model and tokenizer\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {response}\\nJaccard: {jaccard}\\nHallucination Score: {score:.3f} (0=low, 1=high)\\n---\")\n",
    "else:\n",
    "    print(\"Model or tokenizer failed to load. Check your training completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8262656,
     "sourceId": 13048307,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8266562,
     "sourceId": 13054375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
